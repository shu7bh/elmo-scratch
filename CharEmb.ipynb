{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'dev_train_len': 25*10**3,\n",
    "    'dev_validation_len': 5*10**3,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 100,\n",
    "    'char_embedding_dim': 32,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.1,\n",
    "    'optimizer': 'Adam',\n",
    "    'num_layers': 2,\n",
    "    'word_emb_dim': 200,\n",
    "    'max_word_len': 20,\n",
    "    'hidden_dim': 100,\n",
    "    'char_out_channels': 64,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_TRAIN_LEN = cfg['dev_train_len']\n",
    "DEV_VALIDATION_LEN = cfg['dev_validation_len']\n",
    "LEARNING_RATE = cfg['learning_rate']\n",
    "EPOCHS = cfg['epochs']\n",
    "CHAR_EMBEDDING_DIM = cfg['char_embedding_dim']\n",
    "BATCH_SIZE = cfg['batch_size']\n",
    "DROPOUT = cfg['dropout']\n",
    "OPTIMIZER = cfg['optimizer']\n",
    "NUM_LAYERS = cfg['num_layers']\n",
    "HIDDEN_DIM = cfg['hidden_dim']\n",
    "WORD_EMB_DIM = cfg['word_emb_dim']\n",
    "MAX_WORD_LEN = cfg['max_word_len']\n",
    "CHAR_OUT_CHANNELS = cfg['char_out_channels']\n",
    "\n",
    "DIR = '/scratch/shu7bh/RES/4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "unique_chars = set()\n",
    "\n",
    "def clean_data(text: str) -> str:\n",
    "    text = normalize_unicode(text.lower().strip())\n",
    "    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n",
    "    for char in text:\n",
    "        unique_chars.add(char)\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "freq_words = dict()\n",
    "def tokenize_data(text: str, create_unique_words: bool) -> list:\n",
    "    global freq_words\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "    if create_unique_words:\n",
    "        for token in tokens:\n",
    "            if token not in freq_words:\n",
    "                freq_words[token] = 1\n",
    "            else:\n",
    "                freq_words[token] += 1\n",
    "    return tokens\n",
    "\n",
    "def replace_words(tokens: list, filter_rare_words: bool) -> list:\n",
    "    new_tokens = []\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in freq_words:\n",
    "            new_tokens.append('<unk>')\n",
    "        else:\n",
    "            if filter_rare_words:\n",
    "                if freq_words[tokens[i]] < 4:\n",
    "                    new_tokens.append('<unk>')\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "    return new_tokens\n",
    "\n",
    "def read_data(path: str, create_unique_words, filter_rare_words) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    df['Description'] = df['Description'].apply(clean_data)\n",
    "    df['Description'] = df['Description'].apply(tokenize_data, create_unique_words=create_unique_words)\n",
    "    df['Class Index'] = df['Class Index'].apply(lambda x: x-1)\n",
    "    pred_df = df.copy(deep=True)\n",
    "    pred_df['Description'] = pred_df['Description'].apply(replace_words, filter_rare_words=filter_rare_words)\n",
    "    return df, pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23765\n"
     ]
    }
   ],
   "source": [
    "freq_words = dict()\n",
    "actual_df, pred_df = read_data(\n",
    "    'data/train.csv', \n",
    "    create_unique_words=True, \n",
    "    filter_rare_words=True\n",
    ")\n",
    "\n",
    "unique_words = set()\n",
    "for tokens in pred_df['Description']:\n",
    "    unique_words.update(tokens)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(set(actual_df['Class Index']))\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Char -> Idx and Word -> Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': 1, 'e': 2, '!': 3, 'q': 4, 'c': 5, '.': 6, '?': 7, 'w': 8, ' ': 9, 'm': 10, 'y': 11, 'i': 12, 'a': 13, 'l': 14, 'p': 15, 'j': 16, 'v': 17, 'r': 18, 'g': 19, 'z': 20, 'u': 21, 'o': 22, 'k': 23, 'n': 24, 't': 25, 's': 26, 'x': 27, 'h': 28, 'f': 29, 'd': 30, '<pad>': 0, '<sos>': 31, '<eos>': 32}\n",
      "{1: 'b', 2: 'e', 3: '!', 4: 'q', 5: 'c', 6: '.', 7: '?', 8: 'w', 9: ' ', 10: 'm', 11: 'y', 12: 'i', 13: 'a', 14: 'l', 15: 'p', 16: 'j', 17: 'v', 18: 'r', 19: 'g', 20: 'z', 21: 'u', 22: 'o', 23: 'k', 24: 'n', 25: 't', 26: 's', 27: 'x', 28: 'h', 29: 'f', 30: 'd', 0: '<pad>', 31: '<sos>', 32: '<eos>'}\n",
      "23768\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of all characters\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n",
    "\n",
    "# Add special tokens\n",
    "char_to_idx['<pad>'] = 0\n",
    "char_to_idx['<sos>'] = len(char_to_idx)\n",
    "char_to_idx['<eos>'] = len(char_to_idx)\n",
    "\n",
    "# Create a dictionary of all characters\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "# print the character to index mapping\n",
    "print(char_to_idx)\n",
    "print(idx_to_char)\n",
    "\n",
    "# Create a dictionary of all words\n",
    "word_to_idx = {word: idx + 1 for idx, word in enumerate(unique_words)}\n",
    "\n",
    "# Add special tokens\n",
    "word_to_idx['<pad>'] = 0\n",
    "word_to_idx['<sos>'] = len(word_to_idx)\n",
    "word_to_idx['<eos>'] = len(word_to_idx)\n",
    "\n",
    "# Create a dictionary of all words\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# print the length of the word to index mapping\n",
    "print(len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_train_raw_a = actual_df[:DEV_TRAIN_LEN]\n",
    "dev_train_raw_p = pred_df[:DEV_TRAIN_LEN]\n",
    "\n",
    "dev_validation_raw_a = actual_df[DEV_TRAIN_LEN:DEV_TRAIN_LEN+DEV_VALIDATION_LEN]\n",
    "dev_validation_raw_p = pred_df[DEV_TRAIN_LEN:DEV_TRAIN_LEN+DEV_VALIDATION_LEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Sentences(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            adf: pd.DataFrame, \n",
    "            pdf: pd.DataFrame, \n",
    "            char_to_idx: dict, \n",
    "            word_to_idx: dict\n",
    "        ) -> None:\n",
    "\n",
    "        self.X = []\n",
    "        self.Y_ = []\n",
    "\n",
    "        for sentence in adf['Description']:\n",
    "            sent = []\n",
    "            for w in sentence:\n",
    "                sent += [[char_to_idx[w[i]] for i in range(min(MAX_WORD_LEN, len(w)))] + [char_to_idx['<pad>']] * (MAX_WORD_LEN - len(w))]\n",
    "\n",
    "            sent += [[char_to_idx['<eos>']] * MAX_WORD_LEN]\n",
    "            sent = torch.cat([torch.tensor(word) for word in sent])\n",
    "            self.X += [sent]\n",
    "\n",
    "        for sentence in pdf['Description']:\n",
    "            self.Y_ += [torch.tensor([word_to_idx['<sos>']] + [word_to_idx[w] for w in sentence] + [word_to_idx['<eos>']])]\n",
    "\n",
    "        self.Y = torch.tensor(adf['Class Index'].tolist())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        return self.X[idx], self.Y[idx], torch.tensor(len(self.X[idx])), self.Y_[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_train_dataset = Sentences(dev_train_raw_a, dev_train_raw_p, char_to_idx, word_to_idx)\n",
    "dev_validation_dataset = Sentences(dev_validation_raw_a, dev_validation_raw_p, char_to_idx, word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: list) -> tuple:\n",
    "    x, y, l, y_ = zip(*batch)\n",
    "\n",
    "    x = torch.nn.utils.rnn.pad_sequence(x, padding_value=char_to_idx['<pad>'], batch_first=True)\n",
    "    y_ = torch.nn.utils.rnn.pad_sequence(y_, padding_value=word_to_idx['<pad>'], batch_first=True)\n",
    "    y_ = torch.cat([y_, torch.zeros(y_.shape[0], 1, dtype=torch.long)], dim=1)\n",
    "    l = [i/20 for i in l]\n",
    "    return x, torch.stack(y), torch.stack(l), y_[..., 2:], y_[..., :-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dev_train_loader = DataLoader(dev_train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "dev_validation_loader = DataLoader(dev_validation_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CharCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharCNN(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            char_vocab: int,\n",
    "            char_embed_dim: int,\n",
    "            char_out_channels: list,\n",
    "            char_kernel_sizes: list,\n",
    "            dropout: float,\n",
    "            word_embed_dim: int\n",
    "        ) -> None:\n",
    "\n",
    "        super(CharCNN, self).__init__()\n",
    "\n",
    "        self.char_embed = nn.Embedding(char_vocab, char_embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv_max_pools = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(char_embed_dim, char_out_channels[i], char_kernel_sizes[i]),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool1d(1),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            for i in range(len(char_out_channels))\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(sum(char_out_channels), word_embed_dim) # the fully connected layer\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.char_embed(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = [conv_max_pool(x) for conv_max_pool in self.conv_max_pools]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMo(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            char_vocab: int, \n",
    "            char_embed_dim: int, \n",
    "            char_out_channels: list, \n",
    "            char_kernel_sizes: list, \n",
    "            dropout: float, \n",
    "            num_layers: int, \n",
    "            hidden_dim: int, \n",
    "            word_embed_dim: int,\n",
    "            filename: str = None\n",
    "        ) -> None:\n",
    "\n",
    "        super(ELMo, self).__init__()\n",
    "\n",
    "        self.char_cnn = CharCNN(\n",
    "            char_vocab=char_vocab, \n",
    "            char_embed_dim=char_embed_dim, \n",
    "            char_out_channels=char_out_channels, \n",
    "            char_kernel_sizes=char_kernel_sizes, \n",
    "            dropout=dropout,\n",
    "            word_embed_dim=word_embed_dim\n",
    "        )\n",
    "\n",
    "        self.lstmf = nn.LSTM(\n",
    "            input_size=word_embed_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.lstmb = nn.LSTM(\n",
    "            input_size=word_embed_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        if filename:\n",
    "            self.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, l: torch.Tensor) -> torch.Tensor:\n",
    "        bz = x.shape[0]\n",
    "        x = x.view(-1, MAX_WORD_LEN)\n",
    "        x = self.char_cnn(x)\n",
    "        x = x.view(bz, -1, x.shape[1])\n",
    "        xf = x\n",
    "        xb = x.flip([1])\n",
    "        input = x.detach().clone()\n",
    "        xf, (hsf, csf) = self.lstmf(xf)\n",
    "        xb, (hsb, csb) = self.lstmb(xb)\n",
    "        xb = xb.flip([1])\n",
    "        xf = self.dropout(xf)\n",
    "        xb = self.dropout(xb)\n",
    "        return xf, xb, input, (hsf, csf), (hsb, csb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience:int = 3, delta:float = 0.001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss:float = np.inf\n",
    "        self.best_model_pth = 0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, loss, epoch: int):\n",
    "        should_stop = False\n",
    "\n",
    "        if loss >= self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                should_stop = True\n",
    "        else:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.best_model_pth = epoch\n",
    "        return should_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class LM(nn.Module):\n",
    "    def __init__(self, \n",
    "            char_vocab: int,\n",
    "            hidden_dim: int, \n",
    "            vocab_size: int, \n",
    "            filename: str = None\n",
    "        ) -> None:\n",
    "\n",
    "        super(LM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.elmo = ELMo(\n",
    "            char_vocab=char_vocab, \n",
    "            char_embed_dim=CHAR_EMBEDDING_DIM, \n",
    "            char_out_channels=[CHAR_OUT_CHANNELS] * 5,\n",
    "            char_kernel_sizes=[2, 3, 4, 5, 6], \n",
    "            dropout=DROPOUT, \n",
    "            num_layers=NUM_LAYERS, \n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            word_embed_dim=WORD_EMB_DIM\n",
    "        )\n",
    "        self.linear_forward = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.linear_backward = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        if filename:\n",
    "            self.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, l: torch.Tensor) -> torch.Tensor:\n",
    "        xf, xb, _, _, _ = self.elmo(x, l)\n",
    "        yf = self.linear_forward(xf)\n",
    "        yb = self.linear_backward(xb)\n",
    "        return yf, yb\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, validation_loader: DataLoader, epochs: int, learning_rate: float, filename: str) -> None:\n",
    "        self.es = EarlyStopping()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<pad>'])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print('----------------------------------------')\n",
    "            self._train(train_loader)\n",
    "            loss = self._evaluate(validation_loader)\n",
    "            print(f'Epoch: {epoch + 1} | Loss: {loss:7.4f}')\n",
    "            if self.es(loss, epoch):\n",
    "                break\n",
    "            if self.es.counter == 0:\n",
    "                torch.save(self.state_dict(), os.path.join(DIR, f'{filename}_lm.pth'))\n",
    "                torch.save(self.elmo.state_dict(), os.path.join(DIR, f'{filename}_elmo.pth'))\n",
    "                torch.save(self.elmo.char_cnn.state_dict(), os.path.join(DIR, f'{filename}_char_cnn.pth'))\n",
    "\n",
    "    def _call(self, x: torch.Tensor, y: torch.Tensor, l: torch.Tensor, yf: torch.Tensor, yb: torch.Tensor) -> torch.Tensor:\n",
    "        x, y, yf, yb = x.to(DEVICE), y.to(DEVICE), yf.to(DEVICE), yb.to(DEVICE)\n",
    "        # print(x.shape, y.shape, l.shape, yf.shape, yb.shape)\n",
    "        yf_hat, yb_hat = self(x, l)\n",
    "\n",
    "        yf_hat = yf_hat.view(-1, self.vocab_size)\n",
    "        yb_hat = yb_hat.view(-1, self.vocab_size)\n",
    "\n",
    "        yf = yf.view(-1)\n",
    "        yb = yb.view(-1)\n",
    "\n",
    "        # print(yf_hat.shape, yb_hat.shape, yf.shape, yb.shape)\n",
    "\n",
    "        loss1 = self.criterion(yf_hat, yf)\n",
    "        loss2 = self.criterion(yb_hat, yb)\n",
    "\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        # print(loss1.item(), loss2.item(), loss.item())\n",
    "        return loss, loss1, loss2\n",
    "\n",
    "    def _train(self, train_loader: DataLoader) -> None:\n",
    "        self.train()\n",
    "        epoch_loss = []\n",
    "        epoch_loss1 = []\n",
    "        epoch_loss2 = []\n",
    "\n",
    "        pbar = tqdm(train_loader)\n",
    "        for x, y, l, yf, yb in pbar:\n",
    "\n",
    "            loss, loss1, loss2 = self._call(x, y, l, yf, yb)\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_loss1.append(loss1.item())\n",
    "            epoch_loss2.append(loss2.item())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pbar.set_description(f'T Loss: {loss.item():7.4f}, Avg Loss: {np.mean(epoch_loss):7.4f}, Avg Loss1: {np.mean(epoch_loss1):7.4f}, Avg Loss2: {np.mean(epoch_loss2):7.4f}')\n",
    "\n",
    "        # run.log({'upstream_train_loss': np.mean(epoch_loss)})\n",
    "\n",
    "    def _evaluate(self, validation_loader: DataLoader) -> float:\n",
    "        self.eval()\n",
    "        epoch_loss = []\n",
    "        epoch_loss1 = []\n",
    "        epoch_loss2 = []\n",
    "        pbar = tqdm(validation_loader)\n",
    "        with torch.no_grad():\n",
    "            for x, y, l, yf, yb in pbar:\n",
    "                loss, loss1, loss2 = self._call(x, y, l, yf, yb)\n",
    "                epoch_loss.append(loss.item())\n",
    "                epoch_loss1.append(loss1.item())\n",
    "                epoch_loss2.append(loss2.item())\n",
    "                pbar.set_description(f'V Loss: {epoch_loss[-1]:7.4f}, Avg Loss: {np.mean(epoch_loss):7.4f}, Avg Loss1: {np.mean(epoch_loss1):7.4f}, Avg Loss2: {np.mean(epoch_loss2):7.4f}, Counter: {self.es.counter}, Best Loss: {self.es.best_loss:7.4f}')\n",
    "\n",
    "        # run.log({'upstream_validation_loss': np.mean(epoch_loss)})\n",
    "        return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM(\n",
      "  (elmo): ELMo(\n",
      "    (char_cnn): CharCNN(\n",
      "      (char_embed): Embedding(33, 32)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (conv_max_pools): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv1d(32, 64, kernel_size=(2,), stride=(1,))\n",
      "          (1): ReLU()\n",
      "          (2): AdaptiveAvgPool1d(output_size=1)\n",
      "          (3): Flatten(start_dim=1, end_dim=-1)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "          (1): ReLU()\n",
      "          (2): AdaptiveAvgPool1d(output_size=1)\n",
      "          (3): Flatten(start_dim=1, end_dim=-1)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv1d(32, 64, kernel_size=(4,), stride=(1,))\n",
      "          (1): ReLU()\n",
      "          (2): AdaptiveAvgPool1d(output_size=1)\n",
      "          (3): Flatten(start_dim=1, end_dim=-1)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
      "          (1): ReLU()\n",
      "          (2): AdaptiveAvgPool1d(output_size=1)\n",
      "          (3): Flatten(start_dim=1, end_dim=-1)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): Conv1d(32, 64, kernel_size=(6,), stride=(1,))\n",
      "          (1): ReLU()\n",
      "          (2): AdaptiveAvgPool1d(output_size=1)\n",
      "          (3): Flatten(start_dim=1, end_dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (fc): Linear(in_features=320, out_features=200, bias=True)\n",
      "    )\n",
      "    (lstmf): LSTM(200, 100, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (lstmb): LSTM(200, 100, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (linear_forward): Linear(in_features=100, out_features=23768, bias=True)\n",
      "  (linear_backward): Linear(in_features=100, out_features=23768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lm = LM(char_vocab=len(char_to_idx), hidden_dim=HIDDEN_DIM, vocab_size=len(word_to_idx)).to(DEVICE)\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "LM                                                 --\n",
       "├─ELMo: 1-1                                        --\n",
       "│    └─CharCNN: 2-1                                --\n",
       "│    │    └─Embedding: 3-1                         1,056\n",
       "│    │    └─Dropout: 3-2                           --\n",
       "│    │    └─ModuleList: 3-3                        41,280\n",
       "│    │    └─Linear: 3-4                            64,200\n",
       "│    └─LSTM: 2-2                                   201,600\n",
       "│    └─LSTM: 2-3                                   201,600\n",
       "│    └─Dropout: 2-4                                --\n",
       "├─Linear: 1-2                                      2,400,568\n",
       "├─Linear: 1-3                                      2,400,568\n",
       "===========================================================================\n",
       "Total params: 5,310,872\n",
       "Trainable params: 5,310,872\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(lm, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.0533, Avg Loss:  7.0733, Avg Loss1:  7.1113, Avg Loss2:  7.0354: 100%|██████████| 782/782 [01:15<00:00, 10.31it/s]\n",
      "V Loss:  6.8613, Avg Loss:  6.9205, Avg Loss1:  6.9612, Avg Loss2:  6.8797, Counter: 0, Best Loss:     inf: 100%|██████████| 157/157 [00:02<00:00, 60.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss:  6.9205\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.0013, Avg Loss:  6.8521, Avg Loss1:  6.9383, Avg Loss2:  6.7659: 100%|██████████| 782/782 [00:25<00:00, 30.44it/s]\n",
      "V Loss:  6.9507, Avg Loss:  6.8327, Avg Loss1:  6.9441, Avg Loss2:  6.7212, Counter: 0, Best Loss:  6.9205: 100%|██████████| 157/157 [00:02<00:00, 68.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss:  6.8327\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.6718, Avg Loss:  6.8010, Avg Loss1:  6.9115, Avg Loss2:  6.6904: 100%|██████████| 782/782 [00:25<00:00, 30.62it/s]\n",
      "V Loss:  6.7583, Avg Loss:  6.8078, Avg Loss1:  6.9297, Avg Loss2:  6.6859, Counter: 0, Best Loss:  6.8327: 100%|██████████| 157/157 [00:02<00:00, 66.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss:  6.8078\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.8191, Avg Loss:  6.7790, Avg Loss1:  6.8954, Avg Loss2:  6.6626: 100%|██████████| 782/782 [00:25<00:00, 30.19it/s]\n",
      "V Loss:  6.9024, Avg Loss:  6.7941, Avg Loss1:  6.9150, Avg Loss2:  6.6733, Counter: 0, Best Loss:  6.8078: 100%|██████████| 157/157 [00:02<00:00, 65.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss:  6.7941\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.2997, Avg Loss:  6.5955, Avg Loss1:  6.6506, Avg Loss2:  6.5405: 100%|██████████| 782/782 [00:25<00:00, 30.12it/s]\n",
      "V Loss:  6.2063, Avg Loss:  6.4784, Avg Loss1:  6.5227, Avg Loss2:  6.4340, Counter: 0, Best Loss:  6.7941: 100%|██████████| 157/157 [00:02<00:00, 67.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss:  6.4784\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.4998, Avg Loss:  6.3253, Avg Loss1:  6.3498, Avg Loss2:  6.3009: 100%|██████████| 782/782 [00:25<00:00, 30.43it/s]\n",
      "V Loss:  6.3635, Avg Loss:  6.2568, Avg Loss1:  6.2710, Avg Loss2:  6.2425, Counter: 0, Best Loss:  6.4784: 100%|██████████| 157/157 [00:02<00:00, 66.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss:  6.2568\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.3647, Avg Loss:  6.1317, Avg Loss1:  6.1296, Avg Loss2:  6.1338: 100%|██████████| 782/782 [00:25<00:00, 30.90it/s]\n",
      "V Loss:  6.1479, Avg Loss:  6.1018, Avg Loss1:  6.1047, Avg Loss2:  6.0990, Counter: 0, Best Loss:  6.2568: 100%|██████████| 157/157 [00:02<00:00, 66.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss:  6.1018\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.6913, Avg Loss:  5.9859, Avg Loss1:  5.9737, Avg Loss2:  5.9981: 100%|██████████| 782/782 [00:25<00:00, 30.58it/s]\n",
      "V Loss:  6.2949, Avg Loss:  5.9927, Avg Loss1:  5.9894, Avg Loss2:  5.9959, Counter: 0, Best Loss:  6.1018: 100%|██████████| 157/157 [00:02<00:00, 66.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss:  5.9927\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.0377, Avg Loss:  5.8671, Avg Loss1:  5.8487, Avg Loss2:  5.8856: 100%|██████████| 782/782 [00:25<00:00, 30.88it/s]\n",
      "V Loss:  6.0362, Avg Loss:  5.8980, Avg Loss1:  5.8934, Avg Loss2:  5.9025, Counter: 0, Best Loss:  5.9927: 100%|██████████| 157/157 [00:02<00:00, 66.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss:  5.8980\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.8567, Avg Loss:  5.7675, Avg Loss1:  5.7434, Avg Loss2:  5.7917: 100%|██████████| 782/782 [00:25<00:00, 30.76it/s]\n",
      "V Loss:  6.1588, Avg Loss:  5.8260, Avg Loss1:  5.8190, Avg Loss2:  5.8329, Counter: 0, Best Loss:  5.8980: 100%|██████████| 157/157 [00:02<00:00, 66.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss:  5.8260\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.5194, Avg Loss:  5.6797, Avg Loss1:  5.6497, Avg Loss2:  5.7096: 100%|██████████| 782/782 [00:25<00:00, 30.70it/s]\n",
      "V Loss:  5.9024, Avg Loss:  5.7685, Avg Loss1:  5.7582, Avg Loss2:  5.7788, Counter: 0, Best Loss:  5.8260: 100%|██████████| 157/157 [00:02<00:00, 67.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Loss:  5.7685\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.7706, Avg Loss:  5.6027, Avg Loss1:  5.5705, Avg Loss2:  5.6350: 100%|██████████| 782/782 [00:25<00:00, 30.73it/s]\n",
      "V Loss:  5.5917, Avg Loss:  5.7080, Avg Loss1:  5.7015, Avg Loss2:  5.7144, Counter: 0, Best Loss:  5.7685: 100%|██████████| 157/157 [00:02<00:00, 69.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Loss:  5.7080\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  4.6026, Avg Loss:  5.5298, Avg Loss1:  5.4983, Avg Loss2:  5.5612: 100%|██████████| 782/782 [00:25<00:00, 30.63it/s]\n",
      "V Loss:  5.9076, Avg Loss:  5.6662, Avg Loss1:  5.6613, Avg Loss2:  5.6712, Counter: 0, Best Loss:  5.7080: 100%|██████████| 157/157 [00:02<00:00, 70.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Loss:  5.6662\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.4133, Avg Loss:  5.4656, Avg Loss1:  5.4360, Avg Loss2:  5.4952: 100%|██████████| 782/782 [00:25<00:00, 30.90it/s]\n",
      "V Loss:  5.7122, Avg Loss:  5.6254, Avg Loss1:  5.6246, Avg Loss2:  5.6261, Counter: 0, Best Loss:  5.6662: 100%|██████████| 157/157 [00:02<00:00, 67.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Loss:  5.6254\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.3186, Avg Loss:  5.4053, Avg Loss1:  5.3764, Avg Loss2:  5.4342: 100%|██████████| 782/782 [00:25<00:00, 30.55it/s]\n",
      "V Loss:  5.5492, Avg Loss:  5.5975, Avg Loss1:  5.5948, Avg Loss2:  5.6002, Counter: 0, Best Loss:  5.6254: 100%|██████████| 157/157 [00:02<00:00, 67.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Loss:  5.5975\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.2540, Avg Loss:  5.3521, Avg Loss1:  5.3254, Avg Loss2:  5.3788: 100%|██████████| 782/782 [00:25<00:00, 30.44it/s]\n",
      "V Loss:  6.1230, Avg Loss:  5.5672, Avg Loss1:  5.5706, Avg Loss2:  5.5637, Counter: 0, Best Loss:  5.5975: 100%|██████████| 157/157 [00:02<00:00, 67.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Loss:  5.5672\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.7274, Avg Loss:  5.3041, Avg Loss1:  5.2789, Avg Loss2:  5.3293: 100%|██████████| 782/782 [00:25<00:00, 30.56it/s]\n",
      "V Loss:  5.4733, Avg Loss:  5.5407, Avg Loss1:  5.5481, Avg Loss2:  5.5332, Counter: 0, Best Loss:  5.5672: 100%|██████████| 157/157 [00:02<00:00, 69.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Loss:  5.5407\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.1112, Avg Loss:  5.2598, Avg Loss1:  5.2351, Avg Loss2:  5.2846: 100%|██████████| 782/782 [00:25<00:00, 30.41it/s]\n",
      "V Loss:  5.7267, Avg Loss:  5.5178, Avg Loss1:  5.5275, Avg Loss2:  5.5081, Counter: 0, Best Loss:  5.5407: 100%|██████████| 157/157 [00:02<00:00, 69.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Loss:  5.5178\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.2569, Avg Loss:  5.2179, Avg Loss1:  5.1946, Avg Loss2:  5.2411: 100%|██████████| 782/782 [00:24<00:00, 31.47it/s]\n",
      "V Loss:  5.2099, Avg Loss:  5.4997, Avg Loss1:  5.5114, Avg Loss2:  5.4880, Counter: 0, Best Loss:  5.5178: 100%|██████████| 157/157 [00:02<00:00, 69.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Loss:  5.4997\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.1919, Avg Loss:  5.1809, Avg Loss1:  5.1589, Avg Loss2:  5.2030: 100%|██████████| 782/782 [00:25<00:00, 31.12it/s]\n",
      "V Loss:  5.4726, Avg Loss:  5.4887, Avg Loss1:  5.5015, Avg Loss2:  5.4759, Counter: 0, Best Loss:  5.4997: 100%|██████████| 157/157 [00:02<00:00, 73.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Loss:  5.4887\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  4.9272, Avg Loss:  5.1445, Avg Loss1:  5.1230, Avg Loss2:  5.1659: 100%|██████████| 782/782 [00:25<00:00, 31.25it/s]\n",
      "V Loss:  6.0672, Avg Loss:  5.4880, Avg Loss1:  5.5017, Avg Loss2:  5.4742, Counter: 0, Best Loss:  5.4887: 100%|██████████| 157/157 [00:02<00:00, 70.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Loss:  5.4880\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.3632, Avg Loss:  5.1144, Avg Loss1:  5.0937, Avg Loss2:  5.1350: 100%|██████████| 782/782 [00:25<00:00, 30.74it/s]\n",
      "V Loss:  5.9468, Avg Loss:  5.4712, Avg Loss1:  5.4865, Avg Loss2:  5.4558, Counter: 1, Best Loss:  5.4887: 100%|██████████| 157/157 [00:02<00:00, 68.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Loss:  5.4712\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.2343, Avg Loss:  5.0835, Avg Loss1:  5.0633, Avg Loss2:  5.1037: 100%|██████████| 782/782 [00:25<00:00, 30.75it/s]\n",
      "V Loss:  4.1086, Avg Loss:  5.4506, Avg Loss1:  5.4678, Avg Loss2:  5.4334, Counter: 0, Best Loss:  5.4712: 100%|██████████| 157/157 [00:02<00:00, 67.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Loss:  5.4506\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.5144, Avg Loss:  5.0557, Avg Loss1:  5.0358, Avg Loss2:  5.0757: 100%|██████████| 782/782 [00:25<00:00, 30.48it/s]\n",
      "V Loss:  5.8351, Avg Loss:  5.4512, Avg Loss1:  5.4670, Avg Loss2:  5.4353, Counter: 0, Best Loss:  5.4506: 100%|██████████| 157/157 [00:02<00:00, 66.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Loss:  5.4512\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.2531, Avg Loss:  5.0294, Avg Loss1:  5.0112, Avg Loss2:  5.0476: 100%|██████████| 782/782 [00:25<00:00, 30.54it/s]\n",
      "V Loss:  5.3601, Avg Loss:  5.4414, Avg Loss1:  5.4581, Avg Loss2:  5.4247, Counter: 1, Best Loss:  5.4506: 100%|██████████| 157/157 [00:02<00:00, 68.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Loss:  5.4414\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  4.9070, Avg Loss:  5.0043, Avg Loss1:  4.9855, Avg Loss2:  5.0231: 100%|██████████| 782/782 [00:25<00:00, 30.78it/s]\n",
      "V Loss:  5.6066, Avg Loss:  5.4422, Avg Loss1:  5.4571, Avg Loss2:  5.4273, Counter: 0, Best Loss:  5.4414: 100%|██████████| 157/157 [00:02<00:00, 69.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Loss:  5.4422\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.2434, Avg Loss:  4.9814, Avg Loss1:  4.9645, Avg Loss2:  4.9983: 100%|██████████| 782/782 [00:25<00:00, 30.39it/s]\n",
      "V Loss:  4.7730, Avg Loss:  5.4338, Avg Loss1:  5.4504, Avg Loss2:  5.4172, Counter: 1, Best Loss:  5.4414: 100%|██████████| 157/157 [00:02<00:00, 68.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Loss:  5.4338\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  4.5018, Avg Loss:  4.9601, Avg Loss1:  4.9423, Avg Loss2:  4.9780: 100%|██████████| 782/782 [00:25<00:00, 30.94it/s]\n",
      "V Loss:  6.3275, Avg Loss:  5.4379, Avg Loss1:  5.4560, Avg Loss2:  5.4199, Counter: 0, Best Loss:  5.4338: 100%|██████████| 157/157 [00:02<00:00, 69.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Loss:  5.4379\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.1042, Avg Loss:  4.9399, Avg Loss1:  4.9228, Avg Loss2:  4.9570: 100%|██████████| 782/782 [00:25<00:00, 30.69it/s]\n",
      "V Loss:  4.8384, Avg Loss:  5.4302, Avg Loss1:  5.4461, Avg Loss2:  5.4143, Counter: 1, Best Loss:  5.4338: 100%|██████████| 157/157 [00:02<00:00, 66.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Loss:  5.4302\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.3182, Avg Loss:  4.9207, Avg Loss1:  4.9037, Avg Loss2:  4.9377: 100%|██████████| 782/782 [00:25<00:00, 30.77it/s]\n",
      "V Loss:  5.7474, Avg Loss:  5.4283, Avg Loss1:  5.4457, Avg Loss2:  5.4109, Counter: 0, Best Loss:  5.4302: 100%|██████████| 157/157 [00:02<00:00, 68.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Loss:  5.4283\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.0406, Avg Loss:  4.9010, Avg Loss1:  4.8839, Avg Loss2:  4.9180: 100%|██████████| 782/782 [00:25<00:00, 31.06it/s]\n",
      "V Loss:  5.1248, Avg Loss:  5.4241, Avg Loss1:  5.4423, Avg Loss2:  5.4060, Counter: 0, Best Loss:  5.4283: 100%|██████████| 157/157 [00:02<00:00, 70.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Loss:  5.4241\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.0918, Avg Loss:  4.8858, Avg Loss1:  4.8694, Avg Loss2:  4.9022: 100%|██████████| 782/782 [00:25<00:00, 30.58it/s]\n",
      "V Loss:  5.4332, Avg Loss:  5.4263, Avg Loss1:  5.4450, Avg Loss2:  5.4076, Counter: 0, Best Loss:  5.4241: 100%|██████████| 157/157 [00:02<00:00, 69.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Loss:  5.4263\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.0259, Avg Loss:  4.8684, Avg Loss1:  4.8511, Avg Loss2:  4.8856: 100%|██████████| 782/782 [00:25<00:00, 30.75it/s]\n",
      "V Loss:  5.4509, Avg Loss:  5.4252, Avg Loss1:  5.4440, Avg Loss2:  5.4064, Counter: 1, Best Loss:  5.4241: 100%|██████████| 157/157 [00:02<00:00, 67.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Loss:  5.4252\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  4.6874, Avg Loss:  4.8523, Avg Loss1:  4.8363, Avg Loss2:  4.8682: 100%|██████████| 782/782 [00:25<00:00, 30.53it/s]\n",
      "V Loss:  5.0181, Avg Loss:  5.4255, Avg Loss1:  5.4438, Avg Loss2:  5.4072, Counter: 2, Best Loss:  5.4241: 100%|██████████| 157/157 [00:02<00:00, 69.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Loss:  5.4255\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.0684, Avg Loss:  4.8379, Avg Loss1:  4.8228, Avg Loss2:  4.8530: 100%|██████████| 782/782 [00:25<00:00, 30.67it/s]\n",
      "V Loss:  5.5330, Avg Loss:  5.4266, Avg Loss1:  5.4429, Avg Loss2:  5.4103, Counter: 3, Best Loss:  5.4241: 100%|██████████| 157/157 [00:02<00:00, 65.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Loss:  5.4266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lm.fit(dev_train_loader, dev_validation_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE, filename='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_train_raw_a = actual_df[DEV_TRAIN_LEN+DEV_VALIDATION_LEN:]\n",
    "downstream_train_raw_p = pred_df[DEV_TRAIN_LEN+DEV_VALIDATION_LEN:]\n",
    "\n",
    "downstream_validation_raw_a = actual_df[:DEV_TRAIN_LEN+DEV_VALIDATION_LEN]\n",
    "downstream_validation_raw_p = pred_df[:DEV_TRAIN_LEN+DEV_VALIDATION_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(len(downstream_train_raw_a))\n",
    "print(len(downstream_validation_raw_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_train_dataset = Sentences(downstream_train_raw_a, downstream_train_raw_p, char_to_idx, word_to_idx)\n",
    "downstream_validation_dataset = Sentences(downstream_validation_raw_a, downstream_validation_raw_p, char_to_idx, word_to_idx)\n",
    "\n",
    "downstream_train_loader = DataLoader(downstream_train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "downstream_validation_loader = DataLoader(downstream_validation_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class NewsClassification(nn.Module):\n",
    "    def __init__(self, \n",
    "            char_vocab: int,\n",
    "            hidden_dim: int, \n",
    "            vocab_size: int, \n",
    "            num_classes: int,\n",
    "            filename: str = None\n",
    "        ) -> None:\n",
    "\n",
    "        super(NewsClassification, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        self.elmo = ELMo(\n",
    "            char_vocab=char_vocab, \n",
    "            char_embed_dim=CHAR_EMBEDDING_DIM, \n",
    "            char_out_channels=[CHAR_OUT_CHANNELS] * 5,\n",
    "            char_kernel_sizes=[2, 3, 4, 5, 6], \n",
    "            dropout=DROPOUT, \n",
    "            num_layers=NUM_LAYERS, \n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            word_embed_dim=WORD_EMB_DIM,\n",
    "            filename=filename\n",
    "        )\n",
    "\n",
    "        for param in self.elmo.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.delta = nn.Parameter(torch.randn(1, 3))\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, l: torch.Tensor) -> torch.Tensor:\n",
    "        _, _, input, (hsf, csf), (hsb, csb) = self.elmo(x, l)\n",
    "        hsf = hsf.permute(1, 0, 2)\n",
    "        csf = csf.permute(1, 0, 2)\n",
    "        hsb = hsb.permute(1, 0, 2)\n",
    "        csb = csb.permute(1, 0, 2)\n",
    "\n",
    "        hs = torch.cat([hsf, hsb], dim=2)\n",
    "        cs = torch.cat([csf, csb], dim=2)\n",
    "\n",
    "        val = (hs + cs) / 2\n",
    "\n",
    "        input = torch.mean(input, dim=1).unsqueeze(1)\n",
    "\n",
    "        val = torch.cat([input, val], dim=1)\n",
    "        val = (self.delta / (torch.sum(self.delta))) @ val\n",
    "        val = val.squeeze()\n",
    "\n",
    "        x = self.linear(val)\n",
    "        return x\n",
    "\n",
    "    def fit(self, \n",
    "            train_loader: DataLoader, \n",
    "            validation_loader: DataLoader, \n",
    "            epochs: int, \n",
    "            learning_rate: float\n",
    "        ) -> None:\n",
    "\n",
    "        self.es = EarlyStopping()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print('----------------------------------------')\n",
    "            self._train(train_loader)\n",
    "            loss = self._evaluate(validation_loader)\n",
    "            print(f'Epoch: {epoch + 1} | Loss: {loss:7.4f}')\n",
    "            if self.es(loss, epoch):\n",
    "                break\n",
    "            if self.es.counter == 0:\n",
    "                torch.save(self.state_dict(), os.path.join(DIR, 'best.pth'))\n",
    "\n",
    "    def _call(self, x: torch.Tensor, y: torch.Tensor, l: torch.Tensor, yf: torch.Tensor, yb: torch.Tensor) -> torch.Tensor:\n",
    "        x, y, yf, yb = x.to(DEVICE), y.to(DEVICE), yf.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "        y_hat = self(x, l)\n",
    "        y_hat = y_hat.view(-1, self.num_classes)\n",
    "        y = y.view(-1)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def _train(self, train_loader: DataLoader) -> None:\n",
    "        self.train()\n",
    "        epoch_loss = []\n",
    "        pbar = tqdm(train_loader)\n",
    "        for x, y, l, yf, yb in pbar:\n",
    "            loss = self._call(x, y, l, yf, yb)\n",
    "            epoch_loss.append(loss.item())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pbar.set_description(f'T Loss: {loss.item():7.4f}, Avg Loss: {np.mean(epoch_loss):7.4f}')\n",
    "\n",
    "    def _evaluate(self, validation_loader: DataLoader) -> float:\n",
    "        self.eval()\n",
    "        epoch_loss = []\n",
    "        pbar = tqdm(validation_loader)\n",
    "        with torch.no_grad():\n",
    "            for x, y, l, yf, yb in pbar:\n",
    "                loss = self._call(x, y, l, yf, yb)\n",
    "                epoch_loss.append(loss.item())\n",
    "                pbar.set_description(f'V Loss: {epoch_loss[-1]:7.4f}, Avg Loss: {np.mean(epoch_loss):7.4f}, Counter: {self.es.counter}, Best Loss: {self.es.best_loss:7.4f}')\n",
    "        return np.mean(epoch_loss)\n",
    "\n",
    "    def _metrics(self, test_loader: DataLoader) -> None:\n",
    "        self.eval()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        pbar = tqdm(test_loader)\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        epoch_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y, l, yf, yb in pbar:\n",
    "                x, y, yf, yb = x.to(DEVICE), y.to(DEVICE), yf.to(DEVICE), yb.to(DEVICE)\n",
    "                y_hat = self(x, l)\n",
    "                y_hat = y_hat.view(-1, self.num_classes)\n",
    "                y = y.view(-1)\n",
    "                loss = self.criterion(y_hat, y)\n",
    "\n",
    "                epoch_loss.append(loss.item())\n",
    "                y_hat = torch.argmax(y_hat, dim=1)\n",
    "                y_pred += y_hat.tolist()\n",
    "                y_true += y.tolist()\n",
    "\n",
    "        print(f'Test Loss: {np.mean(epoch_loss):7.4f}')\n",
    "\n",
    "        cr = classification_report(y_true, y_pred, digits=4)\n",
    "        print('Classification Report:', cr)\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print('Confusion Matrix:', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = NewsClassification(char_vocab=len(char_to_idx), \n",
    "                        hidden_dim=HIDDEN_DIM, \n",
    "                        vocab_size=len(word_to_idx), \n",
    "                        num_classes=NUM_CLASSES,\n",
    "                        filename=os.path.join(DIR, 'best_elmo.pth')\n",
    "                    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "NewsClassification                                 3\n",
       "├─ELMo: 1-1                                        --\n",
       "│    └─CharCNN: 2-1                                --\n",
       "│    │    └─Embedding: 3-1                         (1,056)\n",
       "│    │    └─Dropout: 3-2                           --\n",
       "│    │    └─ModuleList: 3-3                        (41,280)\n",
       "│    │    └─Linear: 3-4                            (64,200)\n",
       "│    └─LSTM: 2-2                                   (201,600)\n",
       "│    └─LSTM: 2-3                                   (201,600)\n",
       "│    └─Dropout: 2-4                                --\n",
       "├─Sequential: 1-2                                  --\n",
       "│    └─Linear: 2-5                                 20,100\n",
       "│    └─ReLU: 2-6                                   --\n",
       "│    └─Linear: 2-7                                 404\n",
       "===========================================================================\n",
       "Total params: 530,243\n",
       "Trainable params: 20,507\n",
       "Non-trainable params: 509,736\n",
       "==========================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(nc, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.7373, Avg Loss:  0.6860: 100%|██████████| 2813/2813 [00:27<00:00, 101.47it/s]\n",
      "V Loss:  0.8952, Avg Loss:  0.6811, Counter: 0, Best Loss:     inf: 100%|██████████| 938/938 [00:06<00:00, 138.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss:  0.6811\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.4789, Avg Loss:  0.6124: 100%|██████████| 2813/2813 [00:25<00:00, 109.14it/s]\n",
      "V Loss:  0.5435, Avg Loss:  0.6229, Counter: 0, Best Loss:  0.6811: 100%|██████████| 938/938 [00:06<00:00, 134.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss:  0.6229\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.3010, Avg Loss:  0.6001: 100%|██████████| 2813/2813 [00:25<00:00, 110.08it/s]\n",
      "V Loss:  0.6057, Avg Loss:  0.5675, Counter: 0, Best Loss:  0.6229: 100%|██████████| 938/938 [00:06<00:00, 137.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss:  0.5675\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.4059, Avg Loss:  0.5863: 100%|██████████| 2813/2813 [00:25<00:00, 108.64it/s]\n",
      "V Loss:  0.7303, Avg Loss:  0.5405, Counter: 0, Best Loss:  0.5675: 100%|██████████| 938/938 [00:06<00:00, 136.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss:  0.5405\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.3019, Avg Loss:  0.5801: 100%|██████████| 2813/2813 [00:25<00:00, 110.14it/s]\n",
      "V Loss:  0.2502, Avg Loss:  0.5444, Counter: 0, Best Loss:  0.5405: 100%|██████████| 938/938 [00:06<00:00, 136.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss:  0.5444\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.5700, Avg Loss:  0.5726: 100%|██████████| 2813/2813 [00:25<00:00, 109.87it/s]\n",
      "V Loss:  0.7550, Avg Loss:  0.5699, Counter: 1, Best Loss:  0.5405: 100%|██████████| 938/938 [00:06<00:00, 137.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss:  0.5699\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.5998, Avg Loss:  0.5726: 100%|██████████| 2813/2813 [00:25<00:00, 110.17it/s]\n",
      "V Loss:  0.5259, Avg Loss:  0.5297, Counter: 2, Best Loss:  0.5405: 100%|██████████| 938/938 [00:06<00:00, 135.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss:  0.5297\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.7394, Avg Loss:  0.5678: 100%|██████████| 2813/2813 [00:25<00:00, 109.29it/s]\n",
      "V Loss:  0.7083, Avg Loss:  0.5312, Counter: 0, Best Loss:  0.5297: 100%|██████████| 938/938 [00:07<00:00, 133.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss:  0.5312\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.5228, Avg Loss:  0.5638: 100%|██████████| 2813/2813 [00:25<00:00, 110.50it/s]\n",
      "V Loss:  0.5998, Avg Loss:  0.5604, Counter: 1, Best Loss:  0.5297: 100%|██████████| 938/938 [00:06<00:00, 136.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss:  0.5604\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.5188, Avg Loss:  0.5640: 100%|██████████| 2813/2813 [00:25<00:00, 110.06it/s]\n",
      "V Loss:  0.7865, Avg Loss:  0.5128, Counter: 2, Best Loss:  0.5297: 100%|██████████| 938/938 [00:06<00:00, 136.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss:  0.5128\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.5591, Avg Loss:  0.5598: 100%|██████████| 2813/2813 [00:25<00:00, 109.48it/s]\n",
      "V Loss:  0.4082, Avg Loss:  0.5271, Counter: 0, Best Loss:  0.5128: 100%|██████████| 938/938 [00:06<00:00, 136.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Loss:  0.5271\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.3950, Avg Loss:  0.5586: 100%|██████████| 2813/2813 [00:25<00:00, 112.10it/s]\n",
      "V Loss:  0.7758, Avg Loss:  0.5335, Counter: 1, Best Loss:  0.5128: 100%|██████████| 938/938 [00:06<00:00, 137.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Loss:  0.5335\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.5361, Avg Loss:  0.5553: 100%|██████████| 2813/2813 [00:25<00:00, 110.61it/s]\n",
      "V Loss:  0.3414, Avg Loss:  0.5346, Counter: 2, Best Loss:  0.5128: 100%|██████████| 938/938 [00:06<00:00, 135.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Loss:  0.5346\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.7613, Avg Loss:  0.5536: 100%|██████████| 2813/2813 [00:25<00:00, 110.67it/s]\n",
      "V Loss:  0.3369, Avg Loss:  0.5249, Counter: 3, Best Loss:  0.5128: 100%|██████████| 938/938 [00:06<00:00, 145.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Loss:  0.5249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nc.fit(downstream_train_loader, downstream_validation_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc.load_state_dict(torch.load(os.path.join(DIR, 'best.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adf, test_pdf = read_data('data/test.csv', create_unique_words=False, filter_rare_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_test_dataset = Sentences(test_adf, test_pdf, char_to_idx, word_to_idx)\n",
    "downstream_test_loader = DataLoader(downstream_test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:01<00:00, 213.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.5309\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8253    0.8253    0.8253      1900\n",
      "           1     0.8318    0.9289    0.8777      1900\n",
      "           2     0.7963    0.6953    0.7423      1900\n",
      "           3     0.7275    0.7347    0.7311      1900\n",
      "\n",
      "    accuracy                         0.7961      7600\n",
      "   macro avg     0.7952    0.7961    0.7941      7600\n",
      "weighted avg     0.7952    0.7961    0.7941      7600\n",
      "\n",
      "Confusion Matrix: [[1568  140   94   98]\n",
      " [  64 1765   23   48]\n",
      " [ 127   75 1321  377]\n",
      " [ 141  142  221 1396]]\n"
     ]
    }
   ],
   "source": [
    "nc._metrics(downstream_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
