{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'dev_train_len': 5*10**3,\n",
    "    'dev_validation_len': 1*10**3,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 100,\n",
    "    'embedding_dim': 50,\n",
    "    'hidden_dim': 50,\n",
    "    'dropout': 0,\n",
    "    'optimizer': 'Adam',\n",
    "    'num_layers': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import unicodedata\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "def tokenize_corpus(s: str) -> str:\n",
    "    s = normalize_unicode(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\"\"[^a-zA-Z0-9?.,;'\"]+\"\"\", \" \", s)\n",
    "    s = re.sub(r'(.)\\1{3,}',r'\\1', s)\n",
    "    s = s.rstrip().strip()\n",
    "    return s\n",
    "\n",
    "def get_word_tokenized_corpus(s: str) -> list:\n",
    "    return word_tokenize(s)\n",
    "\n",
    "glove_dict = {\n",
    "    '50': 'glove-wiki-gigaword-50',\n",
    "    '100': 'glove-wiki-gigaword-100',\n",
    "    '200': 'glove-wiki-gigaword-200'\n",
    "}\n",
    "\n",
    "glove_dict['50'] = api.load(glove_dict['50'])\n",
    "# glove_dict['100'] = api.load(glove_dict['100'])\n",
    "# glove_dict['200'] = api.load(glove_dict['200'])\n",
    "\n",
    "def create_vocab(sentences: list, embedding_dim: int):\n",
    "    glove = glove_dict[str(embedding_dim)]\n",
    "\n",
    "    Emb = KeyedVectors(vector_size=glove.vector_size)\n",
    "    vocab = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        vocab.extend(sentence)\n",
    "\n",
    "    vocab = set(vocab)\n",
    "\n",
    "    vectors, keys = [], []\n",
    "    for token in vocab:\n",
    "        if token in glove:\n",
    "            vectors.append(torch.tensor(glove[token]))\n",
    "            keys.append(token)\n",
    "\n",
    "    keys.extend(['<unk>', '<pad>', '<sos>', '<eos>'])\n",
    "    vectors.append(torch.mean(torch.stack(vectors), dim=0).numpy())\n",
    "    vectors.append([0 for _ in range(embedding_dim)])\n",
    "    vectors.append([random.random() for _ in range(embedding_dim)])\n",
    "    vectors.append([random.random() for _ in range(embedding_dim)])\n",
    "    Emb.add_vectors(keys, vectors)\n",
    "\n",
    "    return Emb\n",
    "\n",
    "def get_sentence_index(sentence: list, Emb: KeyedVectors):\n",
    "    word_vec = []\n",
    "\n",
    "    word_vec.append(Emb.key_to_index['<sos>'])\n",
    "    for word in sentence:\n",
    "        word_vec.append(get_vocab_index(word, Emb))\n",
    "    word_vec.append(Emb.key_to_index['<eos>'])\n",
    "\n",
    "    return torch.tensor(word_vec)\n",
    "\n",
    "def get_vocab_index(word: str, Emb: KeyedVectors):\n",
    "    if word in Emb:\n",
    "        return Emb.key_to_index[word]\n",
    "    return Emb.key_to_index['<unk>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV_TRAIN_LEN = cfg['parameters']['dev_train_len']['value']\n",
    "# DEV_VALIDATION_LEN = cfg['parameters']['dev_validation_len']['value']\n",
    "\n",
    "DEV_TRAIN_LEN = cfg['dev_train_len']\n",
    "DEV_VALIDATION_LEN = cfg['dev_validation_len']\n",
    "\n",
    "DIR = '/scratch/shu7bh/RES/PRE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(DEV_TRAIN_LEN)\n",
    "print(DEV_VALIDATION_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "df['Description'] = df['Description'].apply(tokenize_corpus)\n",
    "df['Description'] = df['Description'].apply(get_word_tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         4\n",
       "1         4\n",
       "2         4\n",
       "3         1\n",
       "4         3\n",
       "         ..\n",
       "119995    4\n",
       "119996    3\n",
       "119997    1\n",
       "119998    4\n",
       "119999    4\n",
       "Name: Class Index, Length: 120000, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_train = df[:DEV_TRAIN_LEN]['Description']\n",
    "dev_validation = df[DEV_TRAIN_LEN:DEV_TRAIN_LEN + DEV_VALIDATION_LEN]['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000    [video, game, publisher, electronic, arts, on,...\n",
       "5001    [by, amanda, gardner, ,, healthday, reporter, ...\n",
       "5002    [percival, became, the, tigers, 39, ;, new, cl...\n",
       "5003    [it, 's, getting, harder, to, shrink, chips, ,...\n",
       "5004    [canadian, press, halifax, cp, they, have, bec...\n",
       "                              ...                        \n",
       "5995    [massive, database, holds, info, on, millions,...\n",
       "5996    [supreme, court, justices, on, tuesday, uncork...\n",
       "5997    [the, yankees, should, soon, clinch, their, se...\n",
       "5998    [the, un, nuclear, agency, agreed, yesterday, ...\n",
       "5999    [ap, how, do, you, explain, a, quarterback, sn...\n",
       "Name: Description, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class ELMO(nn.Module):\n",
    "    def __init__(self, Emb, hidden_dim, dropout, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(Emb.vectors), padding_idx=Emb.key_to_index['<pad>'])\n",
    "        self.lstm = nn.LSTM(Emb.vectors.shape[1], hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        X = self.embedding(X)\n",
    "        X = pack_padded_sequence(X, X_lengths, batch_first=True, enforce_sorted=False)\n",
    "        # X, _ = self.lstm(X, None)\n",
    "        X, (h_n, _) = self.lstm(X, None)\n",
    "        X, _ = pad_packed_sequence(X, batch_first=True)\n",
    "        return X, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LM(nn.Module):\n",
    "    def __init__(self, Emb, hidden_dim, dropout, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.elmo = ELMO(Emb, hidden_dim, dropout, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, Emb.vectors.shape[0])\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        X, _ = self.elmo(X, X_lengths)\n",
    "        X = self.fc(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class SentencesDataset(Dataset):\n",
    "    def __init__(self, sentences: list, Emb):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = []\n",
    "        for sentence in sentences:\n",
    "            self.data.append(get_sentence_index(sentence, Emb))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], torch.tensor(len(self.data[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, Emb):\n",
    "        self.pad_index = Emb.key_to_index['<pad>']\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        X, X_lengths = zip(*batch)\n",
    "        X = pad_sequence(X, batch_first=True, padding_value=self.pad_index)\n",
    "        return X[:, :-1], X[:, 1:], torch.stack(X_lengths) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def fit(model, dataloader, train, es, loss_fn, optimizer):\n",
    "    model.train() if train else model.eval()\n",
    "    epoch_loss = []\n",
    "\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "\n",
    "    for X, Y, X_lengths in pbar:\n",
    "        X = X.to(DEVICE)\n",
    "        Y = Y.to(DEVICE)\n",
    "\n",
    "        Y_pred = model(X, X_lengths)\n",
    "        Y_pred = Y_pred.reshape(-1, Y_pred.shape[2])\n",
    "\n",
    "        Y = Y.reshape(-1)\n",
    "\n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        X.detach()\n",
    "        Y_pred.detach()\n",
    "        Y.detach()\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        pbar.set_description(f'{\"T\" if train else \"V\"} Loss: {loss.item():7.4f}, Avg Loss: {np.mean(epoch_loss):7.4f}, Best Loss: {es.best_loss:7.4f}, Counter: {es.counter}')\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience:int = 3, delta:float = 0.001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss:float = np.inf\n",
    "        self.best_model_pth = 0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, loss, epoch: int):\n",
    "        should_stop = False\n",
    "\n",
    "        if loss >= self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                should_stop = True\n",
    "        else:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.best_model_pth = epoch\n",
    "        return should_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(EPOCHS, model, training_dataloader, validation_dataloader, loss_fn, optimizer):\n",
    "    es = EarlyStopping(patience=1, delta=0.1)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'\\nEpoch {epoch+1}')\n",
    "\n",
    "        epoch_loss = fit(model, training_dataloader, True, es, loss_fn, optimizer)\n",
    "        # wandb.log({'train_loss': epoch_loss})\n",
    "\n",
    "        with torch.no_grad():\n",
    "            epoch_loss = fit(model, validation_dataloader, False, es, loss_fn, optimizer)\n",
    "            # wandb.log({'validation_loss': epoch_loss})\n",
    "            if es(epoch_loss, epoch):\n",
    "                break\n",
    "            if es.counter == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(DIR, f'best_model.pth'))\n",
    "                torch.save(model.elmo.state_dict(), os.path.join(DIR, f'best_model_elmo.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.9441, Avg Loss:  7.5440, Best Loss:     inf, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 45.14it/s]\n",
      "V Loss:  7.0764, Avg Loss:  7.1183, Best Loss:     inf, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 70.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.7168, Avg Loss:  6.8660, Best Loss:  7.1183, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.23it/s]\n",
      "V Loss:  6.7011, Avg Loss:  6.8075, Best Loss:  7.1183, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 79.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.8657, Avg Loss:  6.1721, Best Loss:  6.8075, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.54it/s]\n",
      "V Loss:  6.2183, Avg Loss:  5.9884, Best Loss:  6.8075, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 71.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  4.6929, Avg Loss:  5.4382, Best Loss:  5.9884, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.83it/s]\n",
      "V Loss:  5.2009, Avg Loss:  5.3672, Best Loss:  5.9884, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 72.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  4.5941, Avg Loss:  4.8246, Best Loss:  5.3672, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.66it/s]\n",
      "V Loss:  5.3075, Avg Loss:  4.8565, Best Loss:  5.3672, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 79.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  4.4263, Avg Loss:  4.3121, Best Loss:  4.8565, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.62it/s]\n",
      "V Loss:  4.4044, Avg Loss:  4.4156, Best Loss:  4.8565, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 80.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  3.7449, Avg Loss:  3.8598, Best Loss:  4.4156, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.32it/s]\n",
      "V Loss:  3.8946, Avg Loss:  4.0462, Best Loss:  4.4156, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 74.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  2.9055, Avg Loss:  3.4715, Best Loss:  4.0462, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.98it/s]\n",
      "V Loss:  3.2890, Avg Loss:  3.7275, Best Loss:  4.0462, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 78.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  2.7592, Avg Loss:  3.1337, Best Loss:  3.7275, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 49.33it/s]\n",
      "V Loss:  3.4785, Avg Loss:  3.4716, Best Loss:  3.7275, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 78.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  2.5439, Avg Loss:  2.8317, Best Loss:  3.4716, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.70it/s]\n",
      "V Loss:  3.3278, Avg Loss:  3.2395, Best Loss:  3.4716, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 74.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  2.6278, Avg Loss:  2.5573, Best Loss:  3.2395, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.09it/s]\n",
      "V Loss:  3.0383, Avg Loss:  3.0320, Best Loss:  3.2395, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 73.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  2.4314, Avg Loss:  2.3056, Best Loss:  3.0320, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 49.25it/s]\n",
      "V Loss:  2.7488, Avg Loss:  2.8391, Best Loss:  3.0320, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 74.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  2.3073, Avg Loss:  2.0767, Best Loss:  2.8391, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 47.98it/s]\n",
      "V Loss:  2.6691, Avg Loss:  2.6829, Best Loss:  2.8391, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 74.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.7840, Avg Loss:  1.8694, Best Loss:  2.6829, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 49.49it/s]\n",
      "V Loss:  2.7172, Avg Loss:  2.5355, Best Loss:  2.6829, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 73.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.6992, Avg Loss:  1.6790, Best Loss:  2.5355, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.22it/s]\n",
      "V Loss:  1.9621, Avg Loss:  2.4023, Best Loss:  2.5355, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 75.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.6972, Avg Loss:  1.5075, Best Loss:  2.4023, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 49.47it/s]\n",
      "V Loss:  2.1930, Avg Loss:  2.2933, Best Loss:  2.4023, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 73.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.2564, Avg Loss:  1.3498, Best Loss:  2.2933, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 49.53it/s]\n",
      "V Loss:  2.0299, Avg Loss:  2.1947, Best Loss:  2.2933, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 74.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.2489, Avg Loss:  1.2071, Best Loss:  2.2933, Counter: 1: 100%|██████████| 313/313 [00:06<00:00, 48.57it/s]\n",
      "V Loss:  1.8103, Avg Loss:  2.1081, Best Loss:  2.2933, Counter: 1: 100%|██████████| 63/63 [00:00<00:00, 76.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  1.0416, Avg Loss:  1.0746, Best Loss:  2.1081, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.59it/s]\n",
      "V Loss:  1.8913, Avg Loss:  2.0236, Best Loss:  2.1081, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 70.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.8096, Avg Loss:  0.9540, Best Loss:  2.1081, Counter: 1: 100%|██████████| 313/313 [00:06<00:00, 49.22it/s]\n",
      "V Loss:  1.8867, Avg Loss:  1.9514, Best Loss:  2.1081, Counter: 1: 100%|██████████| 63/63 [00:00<00:00, 72.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.7585, Avg Loss:  0.8428, Best Loss:  1.9514, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 49.10it/s]\n",
      "V Loss:  1.9638, Avg Loss:  1.8796, Best Loss:  1.9514, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 75.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.7479, Avg Loss:  0.7405, Best Loss:  1.9514, Counter: 1: 100%|██████████| 313/313 [00:06<00:00, 49.00it/s]\n",
      "V Loss:  1.9668, Avg Loss:  1.8235, Best Loss:  1.9514, Counter: 1: 100%|██████████| 63/63 [00:00<00:00, 75.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.7127, Avg Loss:  0.6486, Best Loss:  1.8235, Counter: 0: 100%|██████████| 313/313 [00:06<00:00, 48.45it/s]\n",
      "V Loss:  1.9967, Avg Loss:  1.7759, Best Loss:  1.8235, Counter: 0: 100%|██████████| 63/63 [00:00<00:00, 74.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  0.5664, Avg Loss:  0.5654, Best Loss:  1.8235, Counter: 1: 100%|██████████| 313/313 [00:06<00:00, 49.95it/s]\n",
      "V Loss:  1.5640, Avg Loss:  1.7245, Best Loss:  1.8235, Counter: 1: 100%|██████████| 63/63 [00:00<00:00, 73.70it/s]\n"
     ]
    }
   ],
   "source": [
    "def run(config=None):\n",
    "    # with wandb.init(config=cfg):\n",
    "        # config = wandb.config\n",
    "    BATCH_SIZE = 16\n",
    "    # if config.hidden_dim in [300, 500]:\n",
    "    #     BATCH_SIZE = 32\n",
    "    if config['hidden_dim'] in [300, 500]:\n",
    "        BATCH_SIZE = 32\n",
    "\n",
    "    # wandb.log({'batch_size': BATCH_SIZE})\n",
    "    HIDDEN_DIM = config['hidden_dim']\n",
    "    DROP_OUT = config['dropout']\n",
    "    OPTIMIZER = config['optimizer']\n",
    "    LEARNING_RATE = config['learning_rate']\n",
    "    EPOCHS = config['epochs']\n",
    "    EMBEDDITNG_DIM = config['embedding_dim']\n",
    "    NUM_LAYERS = config['num_layers']\n",
    "\n",
    "    Emb = create_vocab(df['Description'], EMBEDDITNG_DIM)\n",
    "\n",
    "    dev_train_dataset = SentencesDataset(dev_train, Emb)\n",
    "    dev_validation_dataset = SentencesDataset(dev_validation, Emb)\n",
    "\n",
    "    collate_fn = Collator(Emb)\n",
    "\n",
    "    training_dataloader = DataLoader(dev_train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=4)\n",
    "    validation_dataloader = DataLoader(dev_validation_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=4)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = LM(Emb, HIDDEN_DIM, DROP_OUT, NUM_LAYERS).to(DEVICE)\n",
    "\n",
    "    optimizer = getattr(torch.optim, OPTIMIZER)(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=Emb.key_to_index['<pad>'])\n",
    "\n",
    "    train(EPOCHS, model, training_dataloader, validation_dataloader, loss_fn, optimizer)\n",
    "\n",
    "run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def run_epoch(model, dataloader, loss_fn):\n",
    "    epoch_loss = []\n",
    "\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "\n",
    "    for X, Y, X_lengths in pbar:\n",
    "        X = X.to(DEVICE)\n",
    "        Y = Y.to(DEVICE)\n",
    "\n",
    "        Y_pred = model(X, X_lengths)\n",
    "        Y_pred = Y_pred.reshape(-1, Y_pred.shape[2])\n",
    "\n",
    "        Y = Y.reshape(-1)\n",
    "\n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        pbar.set_description(f'Loss: {loss.item():7.4f}, Avg Loss: {np.mean(epoch_loss):7.4f}')\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(elmo, validation_dataloader, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        elmo.eval()\n",
    "        epoch_loss = run_epoch(elmo, validation_dataloader, loss_fn)\n",
    "        print(f'Validation Loss: {epoch_loss:7.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:  2.0416, Avg Loss:  1.8229: 100%|██████████| 63/63 [00:00<00:00, 80.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss:  1.8229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Emb = create_vocab(df['Description'], cfg['embedding_dim'])\n",
    "\n",
    "# dev_train_dataset = SentencesDataset(dev_train, Emb)\n",
    "dev_validation_dataset = SentencesDataset(dev_validation, Emb)\n",
    "\n",
    "collate_fn = Collator(Emb)\n",
    "\n",
    "# training_dataloader = DataLoader(dev_train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=4)\n",
    "validation_dataloader = DataLoader(dev_validation_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=4)\n",
    "model = LM(Emb, cfg['hidden_dim'], cfg['dropout'], cfg['num_layers']).to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(DIR, 'best_model.pth')))\n",
    "\n",
    "validate(model, validation_dataloader, nn.CrossEntropyLoss(ignore_index=Emb.key_to_index['<pad>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo = ELMO(Emb, cfg['hidden_dim'], cfg['dropout'], cfg['num_layers']).to(DEVICE)\n",
    "\n",
    "# load only elmo from the best model\n",
    "elmo.load_state_dict(torch.load(os.path.join(DIR, 'best_model_elmo.pth')), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownStream(nn.Module):\n",
    "    def __init__(self, elmo, dropout):\n",
    "        super().__init__()\n",
    "        self.elmo = elmo\n",
    "        # freeze the ELMO parameters\n",
    "        for param in self.elmo.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.hidden_dim = self.elmo.hidden_dim\n",
    "        self.num_layers = self.elmo.num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.delta = nn.Parameter(torch.randn(1, self.num_layers + 1))\n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, 4)\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        _, Y = self.elmo(X, X_lengths)\n",
    "        # get the first hidden layer batch_first=True\n",
    "\n",
    "        print(Y.shape)\n",
    "        Y = Y.permute(1, 0, 2).reshape(Y.shape[0], self.num_layers, self.hidden_dim * 2)\n",
    "\n",
    "        X = torch.mean(X, dim=1)\n",
    "\n",
    "        # Y = Y.reshape(Y.shape[0], Y.shape[1], self.num_layers, 2 * self.hidden_dim)\n",
    "        Y = torch.stack([X, Y], dim=1)\n",
    "\n",
    "        # mean dimension 1 to get Y of shape (batch_size, 1, num_layers + 1, 2 * hidden_dim)\n",
    "        # Y = torch.mean(Y, dim=1)\n",
    "\n",
    "        # multiply by delta \n",
    "        Y = Y * (self.delta / torch.sum(self.delta))\n",
    "\n",
    "        # sum over the num_layers dimension\n",
    "        Y = torch.sum(Y, dim=1)\n",
    "\n",
    "        # pass through linear layer\n",
    "        Y = self.linear(Y)\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DownStreamDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, Emb: KeyedVectors):\n",
    "        self.descriptions = df['Description']\n",
    "        self.descriptions = [get_sentence_index(description, Emb) for description in self.descriptions]\n",
    "        # self.df.loc[: 'Description'] = df['Description'].apply(get_sentence_index, Emb=Emb)\n",
    "        self.class_index = df['Class Index']\n",
    "        self.Emb = Emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        l = torch.tensor(len(self.descriptions[idx]))\n",
    "        return self.descriptions[idx], l, torch.tensor(self.class_index[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownStreamCollator:\n",
    "    def __init__(self, Emb):\n",
    "        self.pad_index = Emb.key_to_index['<pad>']\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        X, X_lengths, Y = zip(*batch)\n",
    "        X = pad_sequence(X, batch_first=True, padding_value=self.pad_index)\n",
    "        print(len(X_lengths))\n",
    "        print(len(Y))\n",
    "        print('hello')\n",
    "        return X, torch.stack(X_lengths), torch.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_train = df[DEV_TRAIN_LEN + DEV_VALIDATION_LEN:]\n",
    "downstream_validation = df[:DEV_TRAIN_LEN + DEV_VALIDATION_LEN]\n",
    "\n",
    "downstream_train_dataset = DownStreamDataset(downstream_train, Emb)\n",
    "downstream_validation_dataset = DownStreamDataset(downstream_validation, Emb)\n",
    "\n",
    "downstream_collate_fn = DownStreamCollator(Emb)\n",
    "\n",
    "downstream_training_dataloader = DataLoader(downstream_train_dataset, batch_size=16, shuffle=True, collate_fn=downstream_collate_fn, pin_memory=True)\n",
    "downstream_validation_dataloader = DataLoader(downstream_validation_dataset, batch_size=16, shuffle=True, collate_fn=downstream_collate_fn, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downstream_fit(model, dataloader, train, loss_fn, optimizer):\n",
    "    model.train() if train else model.eval()\n",
    "    epoch_loss = []\n",
    "\n",
    "    pbar = tqdm.tqdm(dataloader)\n",
    "\n",
    "    for X, X_lengths, Y in pbar:\n",
    "        print('hello')\n",
    "        print(X.shape)\n",
    "        print(X_lengths.shape)\n",
    "        print(Y.shape)\n",
    "        X = X.to(DEVICE)\n",
    "        Y = Y.to(DEVICE)\n",
    "\n",
    "        Y_pred = model(X, X_lengths)\n",
    "        Y_pred = Y_pred.reshape(-1, Y_pred.shape[2])\n",
    "\n",
    "        Y = Y.reshape(-1)\n",
    "\n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        X.detach()\n",
    "        Y_pred.detach()\n",
    "        Y.detach()\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        pbar.set_description(f'{\"T\" if train else \"V\"} Loss: {loss.item():7.4f}, Avg Loss: {np.mean(epoch_loss):7.4f}')\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel = DownStream(elmo, cfg['dropout']).to(DEVICE)\n",
    "doptimizer = getattr(torch.optim, cfg['optimizer'])(dmodel.parameters(), lr=cfg['learning_rate'])\n",
    "dloss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def downstream_train_fn(EPOCHS, model, training_dataloader, validation_dataloader, loss_fn, optimizer):\n",
    "    es = EarlyStopping(patience=1, delta=0.1)\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'\\nEpoch {epoch+1}')\n",
    "\n",
    "        epoch_loss = downstream_fit(model, training_dataloader, True, loss_fn, optimizer)\n",
    "        print(f'Train Loss: {epoch_loss:7.4f}')\n",
    "        # wandb.log({'downstream_train_loss': epoch_loss})\n",
    "\n",
    "        with torch.no_grad():\n",
    "            epoch_loss = downstream_fit(model, validation_dataloader, False, loss_fn, optimizer)\n",
    "            print(f'Validation Loss: {epoch_loss:7.4f}')\n",
    "            if es(epoch_loss, epoch):\n",
    "                break\n",
    "            if es.counter == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(DIR, f'downstream_best_model.pth'))\n",
    "            # wandb.log({'downstream_validation_loss': epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7125 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1920",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/pandas/core/indexes/range.py:345\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[1;32m    346\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 1920 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m downstream_train_fn(cfg[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m], dmodel, downstream_training_dataloader, downstream_validation_dataloader, dloss_fn, doptimizer)\n",
      "\u001b[1;32m/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     epoch_loss \u001b[39m=\u001b[39m downstream_fit(model, training_dataloader, \u001b[39mTrue\u001b[39;00m, loss_fn, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTrain Loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m:\u001b[39;00m\u001b[39m7.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# wandb.log({'downstream_train_loss': epoch_loss})\u001b[39;00m\n",
      "\u001b[1;32m/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb Cell 31\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm\u001b[39m.\u001b[39mtqdm(dataloader)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m X, X_lengths, Y \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mhello\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(X\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     l \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdescriptions[idx]))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgnode050/home2/shu7bh/Courses/ANLP/Assignments/2/all.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdescriptions[idx], l, torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_index[idx])\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/pandas/core/series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m   1006\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_value(key)\n\u001b[1;32m   1009\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1010\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/pandas/core/series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1115\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_loc(label)\n\u001b[1;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/main/lib/python3.11/site-packages/pandas/core/indexes/range.py:347\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[1;32m    346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 347\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    349\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1920"
     ]
    }
   ],
   "source": [
    "downstream_train_fn(cfg['epochs'], dmodel, downstream_training_dataloader, downstream_validation_dataloader, dloss_fn, doptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
