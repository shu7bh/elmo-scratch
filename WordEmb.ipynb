{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'dev_train_len': 5*10**3,\n",
    "    'dev_validation_len': 1*10**3,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 100,\n",
    "    'embedding_dim': 50,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.1,\n",
    "    'optimizer': 'Adam',\n",
    "    'num_layers': 2\n",
    "}\n",
    "\n",
    "cfg['hidden_dim'] = cfg['embedding_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_TRAIN_LEN = cfg['dev_train_len']\n",
    "DEV_VALIDATION_LEN = cfg['dev_validation_len']\n",
    "LEARNING_RATE = cfg['learning_rate']\n",
    "EPOCHS = cfg['epochs']\n",
    "BATCH_SIZE = cfg['batch_size']\n",
    "DROPOUT = cfg['dropout']\n",
    "OPTIMIZER = cfg['optimizer']\n",
    "NUM_LAYERS = cfg['num_layers']\n",
    "HIDDEN_DIM = cfg['hidden_dim']\n",
    "EMBEDDING_DIM = cfg['embedding_dim']\n",
    "\n",
    "DIR = '/scratch/shu7bh/RES/Word/1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.downloader import load\n",
    "\n",
    "glove_dict = {\n",
    "    '50': 'glove-wiki-gigaword-50',\n",
    "    '100': 'glove-wiki-gigaword-100',\n",
    "    '200': 'glove-wiki-gigaword-200'\n",
    "}\n",
    "\n",
    "glove_dict[str(EMBEDDING_DIM)] = load(glove_dict[str(EMBEDDING_DIM)])\n",
    "# glove_dict['100'] = api.load(glove_dict['100'])\n",
    "# glove_dict['200'] = api.load(glove_dict['200'])\n",
    "\n",
    "glove = glove_dict[str(EMBEDDING_DIM)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "def clean_data(text: str) -> str:\n",
    "    text = normalize_unicode(text.lower().strip())\n",
    "    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "freq_words = dict()\n",
    "\n",
    "def tokenize_data(text: str, create_unique_words: bool) -> list:\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in word_tokenize(text)]\n",
    "    tokens = [token if token in glove else '<unk>' for token in tokens]\n",
    "\n",
    "    if '<unk>' in tokens:\n",
    "        return tokens\n",
    "\n",
    "    if create_unique_words:\n",
    "        for token in tokens:\n",
    "            if token not in freq_words:\n",
    "                freq_words[token] = 1\n",
    "            else:\n",
    "                freq_words[token] += 1\n",
    "    return tokens\n",
    "\n",
    "def replace_words(tokens: list, filter_rare_words: bool) -> list:\n",
    "    tokens = [token if token in freq_words else '<unk>' for token in tokens]\n",
    "    if filter_rare_words:\n",
    "        tokens = [token if freq_words[token] >= 4 else '<unk>' for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def read_data(path: str, create_unique_words, filter_rare_words) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    df['Description'] = df['Description'].apply(clean_data)\n",
    "    df['Description'] = df['Description'].apply(tokenize_data, create_unique_words=create_unique_words)\n",
    "\n",
    "    df = df[df['Description'].apply(lambda x: '<unk>' not in x)]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    df['Class Index'] = df['Class Index'].apply(lambda x: x-1)\n",
    "    ydf = df.copy(deep=True)\n",
    "    ydf['Description'] = ydf['Description'].apply(replace_words, filter_rare_words=filter_rare_words)\n",
    "    return df, ydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21489\n"
     ]
    }
   ],
   "source": [
    "freq_words = dict()\n",
    "xdf, ydf = read_data(\n",
    "    'data/train.csv', \n",
    "    create_unique_words=True, \n",
    "    filter_rare_words=True\n",
    ")\n",
    "\n",
    "unique_words = set()\n",
    "for tokens in ydf['Description']:\n",
    "    unique_words.update(tokens)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[london, british, airline, magnate, richard, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[regardless, space, competition, are, poised, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[cbs, million, of, folded, paper, crane, flutt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[in, the, time, it, take, you, to, read, this,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[washington, a, highly, classified, u, intelli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101981</th>\n",
       "      <td>1</td>\n",
       "      <td>[toronto, reuters, national, hockey, league, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101982</th>\n",
       "      <td>3</td>\n",
       "      <td>[com, september, am, pt, ., there, s, no, doub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101983</th>\n",
       "      <td>0</td>\n",
       "      <td>[pakistani, security, force, have, arrested, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101984</th>\n",
       "      <td>3</td>\n",
       "      <td>[palmsource, finally, unveiled, it, new, o, ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101985</th>\n",
       "      <td>3</td>\n",
       "      <td>[japanese, electronics, giant, nec, corp, ha, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101986 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class Index                                        Description\n",
       "0                 3  [london, british, airline, magnate, richard, b...\n",
       "1                 3  [regardless, space, competition, are, poised, ...\n",
       "2                 0  [cbs, million, of, folded, paper, crane, flutt...\n",
       "3                 3  [in, the, time, it, take, you, to, read, this,...\n",
       "4                 0  [washington, a, highly, classified, u, intelli...\n",
       "...             ...                                                ...\n",
       "101981            1  [toronto, reuters, national, hockey, league, t...\n",
       "101982            3  [com, september, am, pt, ., there, s, no, doub...\n",
       "101983            0  [pakistani, security, force, have, arrested, m...\n",
       "101984            3  [palmsource, finally, unveiled, it, new, o, ve...\n",
       "101985            3  [japanese, electronics, giant, nec, corp, ha, ...\n",
       "\n",
       "[101986 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(set(xdf['Class Index']))\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21492\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of all words\n",
    "word_to_idx = {word: idx + 1 for idx, word in enumerate(unique_words)}\n",
    "\n",
    "# Add special tokens\n",
    "word_to_idx['<pad>'] = 0\n",
    "word_to_idx['<sos>'] = len(word_to_idx)\n",
    "word_to_idx['<eos>'] = len(word_to_idx)\n",
    "\n",
    "# Create a dictionary of all words\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# print the length of the word to index mapping\n",
    "print(len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_vec = glove.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_key_to_idx = glove.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_key_to_idx['<pad>'] = len(glove_key_to_idx)\n",
    "glove_key_to_idx['<sos>'] = len(glove_key_to_idx)\n",
    "glove_key_to_idx['<eos>'] = len(glove_key_to_idx)\n",
    "glove_key_to_idx['<unk>'] = len(glove_key_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_idx_to_key = {idx: key for key, idx in glove_key_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "glove_idx_to_vec = glove.vectors\n",
    "\n",
    "pad_vec = np.zeros((1, EMBEDDING_DIM))\n",
    "sos_vec = np.random.rand(1, EMBEDDING_DIM)\n",
    "eos_vec = np.random.rand(1, EMBEDDING_DIM)\n",
    "unk_vec = np.mean(glove_idx_to_vec, axis=0, keepdims=True)\n",
    "\n",
    "glove_idx_to_vec = np.concatenate((glove_idx_to_vec, pad_vec, sos_vec, eos_vec, unk_vec), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_train_raw_x = xdf[:DEV_TRAIN_LEN]\n",
    "dev_train_raw_y = ydf[:DEV_TRAIN_LEN]\n",
    "\n",
    "dev_validation_raw_x = xdf[DEV_TRAIN_LEN:DEV_TRAIN_LEN+DEV_VALIDATION_LEN]\n",
    "dev_validation_raw_y = ydf[DEV_TRAIN_LEN:DEV_TRAIN_LEN+DEV_VALIDATION_LEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Sentences(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            adf: pd.DataFrame, \n",
    "            pdf: pd.DataFrame, \n",
    "            word_to_idx: dict,\n",
    "            glove_key_to_idx: dict\n",
    "        ) -> None:\n",
    "\n",
    "        self.X = []\n",
    "        self.Y_ = []\n",
    "\n",
    "        for sentence in adf['Description']:\n",
    "            self.X += [torch.tensor(\n",
    "                [glove_key_to_idx[w] for w in sentence] + \n",
    "                [glove_key_to_idx['<eos>']]\n",
    "            )]\n",
    "\n",
    "        for sentence in pdf['Description']:\n",
    "            self.Y_ += [torch.tensor(\n",
    "                [word_to_idx['<sos>']] + \n",
    "                [word_to_idx[w] for w in sentence] + \n",
    "                [word_to_idx['<eos>']] + \n",
    "                [word_to_idx['<pad>']]\n",
    "            )]\n",
    "\n",
    "        self.Y = torch.tensor(adf['Class Index'].tolist())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        return self.X[idx], self.Y[idx], torch.tensor(len(self.X[idx])), self.Y_[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_train_dataset = Sentences(dev_train_raw_x, dev_train_raw_y, word_to_idx, glove_key_to_idx)\n",
    "dev_validation_dataset = Sentences(dev_validation_raw_x, dev_validation_raw_y, word_to_idx, glove_key_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: list) -> tuple:\n",
    "    x, y, l, y_ = zip(*batch)\n",
    "\n",
    "    x = torch.nn.utils.rnn.pad_sequence(x, padding_value=glove_key_to_idx['<pad>'], batch_first=True)\n",
    "    y_ = torch.nn.utils.rnn.pad_sequence(y_, padding_value=word_to_idx['<pad>'], batch_first=True)\n",
    "    return x, torch.stack(y), torch.stack(l), y_[..., 2:], y_[..., :-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dev_train_loader = DataLoader(dev_train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "dev_validation_loader = DataLoader(dev_validation_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(idx_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class ELMo(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            glove_idx_to_vec: np.ndarray,\n",
    "            idx_to_word: dict,\n",
    "            dropout: float, \n",
    "            num_layers: int, \n",
    "            hidden_dim: int, \n",
    "            word_embed_dim: int,\n",
    "            filename: str = None\n",
    "        ) -> None:\n",
    "\n",
    "        super(ELMo, self).__init__()\n",
    "\n",
    "        self.word_embed = nn.Embedding.from_pretrained(torch.from_numpy(glove_idx_to_vec).float(), padding_idx=glove_key_to_idx['<pad>'])\n",
    "\n",
    "        self.lstmf = nn.LSTM(\n",
    "            input_size=word_embed_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.lstmb = nn.LSTM(\n",
    "            input_size=word_embed_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        if filename:\n",
    "            self.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, l: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.word_embed(x)\n",
    "        input = x.detach().clone()\n",
    "\n",
    "        xf = pack_padded_sequence(x, lengths=l, batch_first=True, enforce_sorted=False)\n",
    "        xb = pack_padded_sequence(x.flip([1]), lengths=l, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        xf, (hsf, csf) = self.lstmf(xf)\n",
    "        xb, (hsb, csb) = self.lstmb(xb)\n",
    "\n",
    "        xf, _ = pad_packed_sequence(xf, batch_first=True)\n",
    "        xb, _ = pad_packed_sequence(xb, batch_first=True)\n",
    "\n",
    "        xb = xb.flip([1])\n",
    "\n",
    "        xf = self.dropout(xf)\n",
    "        xb = self.dropout(xb)\n",
    "        return xf, xb, input, (hsf, csf), (hsb, csb)\n",
    "\n",
    "    def state_dict_custom(self):\n",
    "        state_dict = self.state_dict()\n",
    "        del state_dict['word_embed.weight'] \n",
    "        return state_dict\n",
    "\n",
    "    def save(self, filename: str) -> None:\n",
    "        torch.save(self.state_dict_custom(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience:int = 3, delta:float = 0.001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss:float = np.inf\n",
    "        self.best_model_pth = 0\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, loss, epoch: int):\n",
    "        should_stop = False\n",
    "\n",
    "        if loss >= self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                should_stop = True\n",
    "        else:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "            self.best_model_pth = epoch\n",
    "        return should_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# run = wandb.init(project='ELMo', entity='shu7bh', name='UpStream and DownStream')\n",
    "# config = wandb.config\n",
    "\n",
    "# config.dev_train_len = DEV_TRAIN_LEN\n",
    "# config.dev_validation_len = DEV_VALIDATION_LEN\n",
    "# config.learning_rate = LEARNING_RATE\n",
    "# config.epochs = EPOCHS\n",
    "# config.char_embedding_dim = CHAR_EMBEDDING_DIM\n",
    "# config.batch_size = BATCH_SIZE\n",
    "# config.dropout = DROPOUT\n",
    "# config.optimizer = OPTIMIZER\n",
    "# config.num_layers = NUM_LAYERS\n",
    "# config.word_emb_dim = WORD_EMB_DIM\n",
    "# config.max_word_len = MAX_WORD_LEN\n",
    "# config.hidden_dim = HIDDEN_DIM\n",
    "# config.char_out_channels = CHAR_OUT_CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class LM(nn.Module):\n",
    "    def __init__(self, \n",
    "            hidden_dim: int, \n",
    "            vocab_size: int, \n",
    "            filename: str = None\n",
    "        ) -> None:\n",
    "\n",
    "        super(LM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.elmo = ELMo(\n",
    "            glove_idx_to_vec=glove_idx_to_vec,\n",
    "            idx_to_word=idx_to_word,\n",
    "            dropout=DROPOUT, \n",
    "            num_layers=NUM_LAYERS, \n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            word_embed_dim=EMBEDDING_DIM\n",
    "        )\n",
    "        self.linear_forward = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.linear_backward = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        if filename:\n",
    "            self.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, l: torch.Tensor) -> torch.Tensor:\n",
    "        xf, xb, _, _, _ = self.elmo(x, l)\n",
    "        yf = self.linear_forward(xf)\n",
    "        yb = self.linear_backward(xb)\n",
    "        return yf, yb\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, validation_loader: DataLoader, epochs: int, learning_rate: float, filename: str) -> None:\n",
    "        self.es = EarlyStopping()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<pad>'])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print('----------------------------------------')\n",
    "            self._train(train_loader)\n",
    "            loss = self._evaluate(validation_loader)\n",
    "            print(f'Epoch: {epoch + 1} | Loss: {loss:7.4f}')\n",
    "            if self.es(loss, epoch):\n",
    "                break\n",
    "            if self.es.counter == 0:\n",
    "                # torch.save(self.state_dict(), os.path.join(DIR, f'{filename}_lm.pth'))\n",
    "                # torch.save(self.elmo.state_dict(), os.path.join(DIR, f'{filename}_elmo.pth'))\n",
    "                self.elmo.save(os.path.join(DIR, f'{filename}_elmo.pth'))\n",
    "\n",
    "    def _call(self, x: torch.Tensor, y: torch.Tensor, l: torch.Tensor, yf: torch.Tensor, yb: torch.Tensor) -> torch.Tensor:\n",
    "        x, y, yf, yb = x.to(DEVICE), y.to(DEVICE), yf.to(DEVICE), yb.to(DEVICE)\n",
    "        yf_hat, yb_hat = self(x, l)\n",
    "\n",
    "        yf_hat = yf_hat.view(-1, self.vocab_size)\n",
    "        yb_hat = yb_hat.view(-1, self.vocab_size)\n",
    "\n",
    "        yf = yf.view(-1)\n",
    "        yb = yb.view(-1)\n",
    "\n",
    "        loss1 = self.criterion(yf_hat, yf)\n",
    "        loss2 = self.criterion(yb_hat, yb)\n",
    "\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, loss1, loss2\n",
    "\n",
    "    def _train(self, train_loader: DataLoader) -> None:\n",
    "        self.train()\n",
    "        epoch_loss = []\n",
    "        epoch_loss1 = []\n",
    "        epoch_loss2 = []\n",
    "\n",
    "        pbar = tqdm(train_loader)\n",
    "        for x, y, l, yf, yb in pbar:\n",
    "\n",
    "            loss, loss1, loss2 = self._call(x, y, l, yf, yb)\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_loss1.append(loss1.item())\n",
    "            epoch_loss2.append(loss2.item())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            pbar.set_description(f'T Loss: {loss.item():7.4f}, Avg Loss: {np.mean(epoch_loss):7.4f}, Avg Loss1: {np.mean(epoch_loss1):7.4f}, Avg Loss2: {np.mean(epoch_loss2):7.4f}')\n",
    "\n",
    "        # run.log({'upstream_train_loss': np.mean(epoch_loss)})\n",
    "\n",
    "    def _evaluate(self, validation_loader: DataLoader) -> float:\n",
    "        self.eval()\n",
    "        epoch_loss = []\n",
    "        epoch_loss1 = []\n",
    "        epoch_loss2 = []\n",
    "        pbar = tqdm(validation_loader)\n",
    "        with torch.no_grad():\n",
    "            for x, y, l, yf, yb in pbar:\n",
    "                loss, loss1, loss2 = self._call(x, y, l, yf, yb)\n",
    "                epoch_loss.append(loss.item())\n",
    "                epoch_loss1.append(loss1.item())\n",
    "                epoch_loss2.append(loss2.item())\n",
    "                pbar.set_description(f'V Loss: {epoch_loss[-1]:7.4f}, Avg Loss: {np.mean(epoch_loss):7.4f}, Avg Loss1: {np.mean(epoch_loss1):7.4f}, Avg Loss2: {np.mean(epoch_loss2):7.4f}, Counter: {self.es.counter}, Best Loss: {self.es.best_loss:7.4f}')\n",
    "\n",
    "        # run.log({'upstream_validation_loss': np.mean(epoch_loss)})\n",
    "        return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM(\n",
      "  (elmo): ELMo(\n",
      "    (word_embed): Embedding(400004, 50, padding_idx=400000)\n",
      "    (lstmf): LSTM(50, 50, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (lstmb): LSTM(50, 50, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (linear_forward): Linear(in_features=50, out_features=21492, bias=True)\n",
      "  (linear_backward): Linear(in_features=50, out_features=21492, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lm = LM(\n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    vocab_size=len(word_to_idx), \n",
    "    filename=None\n",
    ").to(DEVICE)\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LM                                       --\n",
       "├─ELMo: 1-1                              --\n",
       "│    └─Embedding: 2-1                    (20,000,200)\n",
       "│    └─LSTM: 2-2                         40,800\n",
       "│    └─LSTM: 2-3                         40,800\n",
       "│    └─Dropout: 2-4                      --\n",
       "├─Linear: 1-2                            1,096,092\n",
       "├─Linear: 1-3                            1,096,092\n",
       "=================================================================\n",
       "Total params: 22,273,984\n",
       "Trainable params: 2,273,784\n",
       "Non-trainable params: 20,000,200\n",
       "================================================================="
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(lm, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.7357, Avg Loss:  8.3316, Avg Loss1:  7.7042, Avg Loss2:  8.9589: 100%|██████████| 157/157 [00:04<00:00, 33.61it/s]\n",
      "V Loss:  8.0127, Avg Loss:  7.9358, Avg Loss1:  7.0450, Avg Loss2:  8.8267, Counter: 0, Best Loss:     inf: 100%|██████████| 32/32 [00:00<00:00, 85.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss:  7.9358\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.6471, Avg Loss:  7.7860, Avg Loss1:  6.9782, Avg Loss2:  8.5938: 100%|██████████| 157/157 [00:04<00:00, 33.73it/s]\n",
      "V Loss:  8.0163, Avg Loss:  7.8747, Avg Loss1:  7.0445, Avg Loss2:  8.7049, Counter: 0, Best Loss:  7.9358: 100%|██████████| 32/32 [00:00<00:00, 86.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss:  7.8747\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.1592, Avg Loss:  7.7499, Avg Loss1:  6.9560, Avg Loss2:  8.5438: 100%|██████████| 157/157 [00:04<00:00, 33.44it/s]\n",
      "V Loss:  7.5219, Avg Loss:  7.8069, Avg Loss1:  7.0528, Avg Loss2:  8.5610, Counter: 0, Best Loss:  7.8747: 100%|██████████| 32/32 [00:00<00:00, 86.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss:  7.8069\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.1676, Avg Loss:  7.6396, Avg Loss1:  6.9479, Avg Loss2:  8.3312: 100%|██████████| 157/157 [00:04<00:00, 33.68it/s]\n",
      "V Loss:  7.2229, Avg Loss:  7.7423, Avg Loss1:  7.0605, Avg Loss2:  8.4241, Counter: 0, Best Loss:  7.8069: 100%|██████████| 32/32 [00:00<00:00, 86.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss:  7.7423\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.6188, Avg Loss:  7.5761, Avg Loss1:  6.9415, Avg Loss2:  8.2106: 100%|██████████| 157/157 [00:04<00:00, 33.54it/s]\n",
      "V Loss:  7.7310, Avg Loss:  7.7299, Avg Loss1:  7.0616, Avg Loss2:  8.3982, Counter: 0, Best Loss:  7.7423: 100%|██████████| 32/32 [00:00<00:00, 84.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss:  7.7299\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.2065, Avg Loss:  7.5033, Avg Loss1:  6.9358, Avg Loss2:  8.0708: 100%|██████████| 157/157 [00:04<00:00, 34.36it/s]\n",
      "V Loss:  7.7913, Avg Loss:  7.7049, Avg Loss1:  7.0665, Avg Loss2:  8.3433, Counter: 0, Best Loss:  7.7299: 100%|██████████| 32/32 [00:00<00:00, 82.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss:  7.7049\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.2917, Avg Loss:  7.4862, Avg Loss1:  6.9304, Avg Loss2:  8.0420: 100%|██████████| 157/157 [00:04<00:00, 33.74it/s]\n",
      "V Loss:  7.3596, Avg Loss:  7.6711, Avg Loss1:  7.0677, Avg Loss2:  8.2745, Counter: 0, Best Loss:  7.7049: 100%|██████████| 32/32 [00:00<00:00, 83.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss:  7.6711\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.0419, Avg Loss:  7.4251, Avg Loss1:  6.9177, Avg Loss2:  7.9325: 100%|██████████| 157/157 [00:04<00:00, 33.78it/s]\n",
      "V Loss:  7.5515, Avg Loss:  7.5772, Avg Loss1:  7.0517, Avg Loss2:  8.1028, Counter: 0, Best Loss:  7.6711: 100%|██████████| 32/32 [00:00<00:00, 84.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss:  7.5772\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.9948, Avg Loss:  7.3812, Avg Loss1:  6.8987, Avg Loss2:  7.8637: 100%|██████████| 157/157 [00:04<00:00, 33.76it/s]\n",
      "V Loss:  7.2113, Avg Loss:  7.5489, Avg Loss1:  7.0312, Avg Loss2:  8.0666, Counter: 0, Best Loss:  7.5772: 100%|██████████| 32/32 [00:00<00:00, 85.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss:  7.5489\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.1530, Avg Loss:  7.3254, Avg Loss1:  6.8762, Avg Loss2:  7.7745: 100%|██████████| 157/157 [00:04<00:00, 33.89it/s]\n",
      "V Loss:  7.1497, Avg Loss:  7.4800, Avg Loss1:  7.0094, Avg Loss2:  7.9505, Counter: 0, Best Loss:  7.5489: 100%|██████████| 32/32 [00:00<00:00, 85.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss:  7.4800\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.0758, Avg Loss:  7.2820, Avg Loss1:  6.8506, Avg Loss2:  7.7135: 100%|██████████| 157/157 [00:04<00:00, 35.65it/s]\n",
      "V Loss:  6.6700, Avg Loss:  7.4620, Avg Loss1:  6.9768, Avg Loss2:  7.9472, Counter: 0, Best Loss:  7.4800: 100%|██████████| 32/32 [00:00<00:00, 83.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Loss:  7.4620\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.3008, Avg Loss:  7.2232, Avg Loss1:  6.8188, Avg Loss2:  7.6277: 100%|██████████| 157/157 [00:04<00:00, 37.96it/s]\n",
      "V Loss:  7.3456, Avg Loss:  7.4064, Avg Loss1:  6.9598, Avg Loss2:  7.8530, Counter: 0, Best Loss:  7.4620: 100%|██████████| 32/32 [00:00<00:00, 83.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Loss:  7.4064\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.0713, Avg Loss:  7.1581, Avg Loss1:  6.7766, Avg Loss2:  7.5396: 100%|██████████| 157/157 [00:04<00:00, 38.14it/s]\n",
      "V Loss:  7.2525, Avg Loss:  7.3298, Avg Loss1:  6.9087, Avg Loss2:  7.7510, Counter: 0, Best Loss:  7.4064: 100%|██████████| 32/32 [00:00<00:00, 85.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Loss:  7.3298\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.0402, Avg Loss:  7.1091, Avg Loss1:  6.7283, Avg Loss2:  7.4898: 100%|██████████| 157/157 [00:04<00:00, 33.92it/s]\n",
      "V Loss:  7.3682, Avg Loss:  7.2713, Avg Loss1:  6.8725, Avg Loss2:  7.6701, Counter: 0, Best Loss:  7.3298: 100%|██████████| 32/32 [00:00<00:00, 86.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Loss:  7.2713\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.8825, Avg Loss:  7.0604, Avg Loss1:  6.6878, Avg Loss2:  7.4330: 100%|██████████| 157/157 [00:04<00:00, 33.74it/s]\n",
      "V Loss:  7.1336, Avg Loss:  7.2402, Avg Loss1:  6.8471, Avg Loss2:  7.6334, Counter: 0, Best Loss:  7.2713: 100%|██████████| 32/32 [00:00<00:00, 85.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Loss:  7.2402\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.8511, Avg Loss:  6.9993, Avg Loss1:  6.6517, Avg Loss2:  7.3469: 100%|██████████| 157/157 [00:04<00:00, 33.57it/s]\n",
      "V Loss:  6.7793, Avg Loss:  7.1651, Avg Loss1:  6.8060, Avg Loss2:  7.5241, Counter: 0, Best Loss:  7.2402: 100%|██████████| 32/32 [00:00<00:00, 85.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Loss:  7.1651\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.7121, Avg Loss:  6.9542, Avg Loss1:  6.6115, Avg Loss2:  7.2968: 100%|██████████| 157/157 [00:04<00:00, 33.54it/s]\n",
      "V Loss:  6.8602, Avg Loss:  7.1449, Avg Loss1:  6.7776, Avg Loss2:  7.5122, Counter: 0, Best Loss:  7.1651: 100%|██████████| 32/32 [00:00<00:00, 82.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Loss:  7.1449\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.8247, Avg Loss:  6.9043, Avg Loss1:  6.5701, Avg Loss2:  7.2385: 100%|██████████| 157/157 [00:04<00:00, 34.27it/s]\n",
      "V Loss:  6.7695, Avg Loss:  7.0722, Avg Loss1:  6.7390, Avg Loss2:  7.4053, Counter: 0, Best Loss:  7.1449: 100%|██████████| 32/32 [00:00<00:00, 87.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Loss:  7.0722\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.6003, Avg Loss:  6.8496, Avg Loss1:  6.5249, Avg Loss2:  7.1742: 100%|██████████| 157/157 [00:04<00:00, 33.79it/s]\n",
      "V Loss:  6.6418, Avg Loss:  7.0376, Avg Loss1:  6.7077, Avg Loss2:  7.3675, Counter: 0, Best Loss:  7.0722: 100%|██████████| 32/32 [00:00<00:00, 84.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Loss:  7.0376\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.4215, Avg Loss:  6.8175, Avg Loss1:  6.4813, Avg Loss2:  7.1538: 100%|██████████| 157/157 [00:04<00:00, 33.66it/s]\n",
      "V Loss:  6.7888, Avg Loss:  7.0102, Avg Loss1:  6.6750, Avg Loss2:  7.3455, Counter: 0, Best Loss:  7.0376: 100%|██████████| 32/32 [00:00<00:00, 84.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Loss:  7.0102\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  7.1230, Avg Loss:  6.7498, Avg Loss1:  6.4400, Avg Loss2:  7.0597: 100%|██████████| 157/157 [00:04<00:00, 33.85it/s]\n",
      "V Loss:  6.6513, Avg Loss:  6.9461, Avg Loss1:  6.6469, Avg Loss2:  7.2453, Counter: 0, Best Loss:  7.0102: 100%|██████████| 32/32 [00:00<00:00, 88.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Loss:  6.9461\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.3518, Avg Loss:  6.6994, Avg Loss1:  6.3956, Avg Loss2:  7.0031: 100%|██████████| 157/157 [00:04<00:00, 33.98it/s]\n",
      "V Loss:  6.7949, Avg Loss:  6.9255, Avg Loss1:  6.6098, Avg Loss2:  7.2412, Counter: 0, Best Loss:  6.9461: 100%|██████████| 32/32 [00:00<00:00, 84.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Loss:  6.9255\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.4426, Avg Loss:  6.6598, Avg Loss1:  6.3513, Avg Loss2:  6.9682: 100%|██████████| 157/157 [00:04<00:00, 33.24it/s]\n",
      "V Loss:  6.6499, Avg Loss:  6.8971, Avg Loss1:  6.5788, Avg Loss2:  7.2154, Counter: 0, Best Loss:  6.9255: 100%|██████████| 32/32 [00:00<00:00, 84.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Loss:  6.8971\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.3408, Avg Loss:  6.6031, Avg Loss1:  6.3077, Avg Loss2:  6.8984: 100%|██████████| 157/157 [00:04<00:00, 33.76it/s]\n",
      "V Loss:  6.5192, Avg Loss:  6.8432, Avg Loss1:  6.5464, Avg Loss2:  7.1400, Counter: 0, Best Loss:  6.8971: 100%|██████████| 32/32 [00:00<00:00, 84.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Loss:  6.8432\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.2900, Avg Loss:  6.5672, Avg Loss1:  6.2677, Avg Loss2:  6.8668: 100%|██████████| 157/157 [00:04<00:00, 33.61it/s]\n",
      "V Loss:  6.7848, Avg Loss:  6.8152, Avg Loss1:  6.5234, Avg Loss2:  7.1070, Counter: 0, Best Loss:  6.8432: 100%|██████████| 32/32 [00:00<00:00, 86.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Loss:  6.8152\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.2401, Avg Loss:  6.5175, Avg Loss1:  6.2255, Avg Loss2:  6.8095: 100%|██████████| 157/157 [00:04<00:00, 33.94it/s]\n",
      "V Loss:  6.5660, Avg Loss:  6.7851, Avg Loss1:  6.4867, Avg Loss2:  7.0835, Counter: 0, Best Loss:  6.8152: 100%|██████████| 32/32 [00:00<00:00, 84.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Loss:  6.7851\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.0176, Avg Loss:  6.4791, Avg Loss1:  6.1829, Avg Loss2:  6.7752: 100%|██████████| 157/157 [00:04<00:00, 33.72it/s]\n",
      "V Loss:  6.4070, Avg Loss:  6.7432, Avg Loss1:  6.4571, Avg Loss2:  7.0293, Counter: 0, Best Loss:  6.7851: 100%|██████████| 32/32 [00:00<00:00, 81.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Loss:  6.7432\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.4902, Avg Loss:  6.4448, Avg Loss1:  6.1468, Avg Loss2:  6.7429: 100%|██████████| 157/157 [00:04<00:00, 32.98it/s]\n",
      "V Loss:  6.2481, Avg Loss:  6.7358, Avg Loss1:  6.4274, Avg Loss2:  7.0442, Counter: 0, Best Loss:  6.7432: 100%|██████████| 32/32 [00:00<00:00, 84.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Loss:  6.7358\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.2315, Avg Loss:  6.3955, Avg Loss1:  6.1072, Avg Loss2:  6.6838: 100%|██████████| 157/157 [00:04<00:00, 33.86it/s]\n",
      "V Loss:  5.8876, Avg Loss:  6.6910, Avg Loss1:  6.4023, Avg Loss2:  6.9798, Counter: 0, Best Loss:  6.7358: 100%|██████████| 32/32 [00:00<00:00, 85.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Loss:  6.6910\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.4070, Avg Loss:  6.3750, Avg Loss1:  6.0689, Avg Loss2:  6.6810: 100%|██████████| 157/157 [00:04<00:00, 33.56it/s]\n",
      "V Loss:  6.0192, Avg Loss:  6.6481, Avg Loss1:  6.3744, Avg Loss2:  6.9218, Counter: 0, Best Loss:  6.6910: 100%|██████████| 32/32 [00:00<00:00, 85.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Loss:  6.6481\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.1001, Avg Loss:  6.3221, Avg Loss1:  6.0287, Avg Loss2:  6.6154: 100%|██████████| 157/157 [00:04<00:00, 34.07it/s]\n",
      "V Loss:  6.8969, Avg Loss:  6.6418, Avg Loss1:  6.3747, Avg Loss2:  6.9090, Counter: 0, Best Loss:  6.6481: 100%|██████████| 32/32 [00:00<00:00, 86.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Loss:  6.6418\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.3737, Avg Loss:  6.2908, Avg Loss1:  5.9956, Avg Loss2:  6.5861: 100%|██████████| 157/157 [00:04<00:00, 34.22it/s]\n",
      "V Loss:  6.3028, Avg Loss:  6.6345, Avg Loss1:  6.3425, Avg Loss2:  6.9265, Counter: 0, Best Loss:  6.6418: 100%|██████████| 32/32 [00:00<00:00, 84.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Loss:  6.6345\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.9302, Avg Loss:  6.2493, Avg Loss1:  5.9581, Avg Loss2:  6.5405: 100%|██████████| 157/157 [00:04<00:00, 34.80it/s]\n",
      "V Loss:  6.5327, Avg Loss:  6.6081, Avg Loss1:  6.3302, Avg Loss2:  6.8860, Counter: 0, Best Loss:  6.6345: 100%|██████████| 32/32 [00:00<00:00, 87.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Loss:  6.6081\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.2055, Avg Loss:  6.2249, Avg Loss1:  5.9278, Avg Loss2:  6.5219: 100%|██████████| 157/157 [00:04<00:00, 35.02it/s]\n",
      "V Loss:  7.0226, Avg Loss:  6.6069, Avg Loss1:  6.3299, Avg Loss2:  6.8840, Counter: 0, Best Loss:  6.6081: 100%|██████████| 32/32 [00:00<00:00, 87.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Loss:  6.6069\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.0545, Avg Loss:  6.1953, Avg Loss1:  5.8958, Avg Loss2:  6.4949: 100%|██████████| 157/157 [00:04<00:00, 34.37it/s]\n",
      "V Loss:  6.3802, Avg Loss:  6.5630, Avg Loss1:  6.2930, Avg Loss2:  6.8331, Counter: 0, Best Loss:  6.6069: 100%|██████████| 32/32 [00:00<00:00, 86.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Loss:  6.5630\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.9455, Avg Loss:  6.1658, Avg Loss1:  5.8642, Avg Loss2:  6.4674: 100%|██████████| 157/157 [00:04<00:00, 34.59it/s]\n",
      "V Loss:  6.2454, Avg Loss:  6.5595, Avg Loss1:  6.2770, Avg Loss2:  6.8421, Counter: 0, Best Loss:  6.5630: 100%|██████████| 32/32 [00:00<00:00, 86.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Loss:  6.5595\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.8889, Avg Loss:  6.1511, Avg Loss1:  5.8357, Avg Loss2:  6.4665: 100%|██████████| 157/157 [00:04<00:00, 34.34it/s]\n",
      "V Loss:  6.1214, Avg Loss:  6.5333, Avg Loss1:  6.2563, Avg Loss2:  6.8103, Counter: 0, Best Loss:  6.5595: 100%|██████████| 32/32 [00:00<00:00, 84.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Loss:  6.5333\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.0339, Avg Loss:  6.1191, Avg Loss1:  5.8099, Avg Loss2:  6.4283: 100%|██████████| 157/157 [00:04<00:00, 34.00it/s]\n",
      "V Loss:  6.3917, Avg Loss:  6.5194, Avg Loss1:  6.2514, Avg Loss2:  6.7873, Counter: 0, Best Loss:  6.5333: 100%|██████████| 32/32 [00:00<00:00, 89.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Loss:  6.5194\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.6737, Avg Loss:  6.0928, Avg Loss1:  5.7804, Avg Loss2:  6.4052: 100%|██████████| 157/157 [00:04<00:00, 34.36it/s]\n",
      "V Loss:  6.4057, Avg Loss:  6.5045, Avg Loss1:  6.2454, Avg Loss2:  6.7637, Counter: 0, Best Loss:  6.5194: 100%|██████████| 32/32 [00:00<00:00, 86.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Loss:  6.5045\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.7361, Avg Loss:  6.0729, Avg Loss1:  5.7557, Avg Loss2:  6.3900: 100%|██████████| 157/157 [00:04<00:00, 33.78it/s]\n",
      "V Loss:  6.5385, Avg Loss:  6.5177, Avg Loss1:  6.2350, Avg Loss2:  6.8003, Counter: 0, Best Loss:  6.5045: 100%|██████████| 32/32 [00:00<00:00, 82.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Loss:  6.5177\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.3386, Avg Loss:  6.0493, Avg Loss1:  5.7343, Avg Loss2:  6.3644: 100%|██████████| 157/157 [00:04<00:00, 34.04it/s]\n",
      "V Loss:  6.6042, Avg Loss:  6.4758, Avg Loss1:  6.2236, Avg Loss2:  6.7279, Counter: 1, Best Loss:  6.5045: 100%|██████████| 32/32 [00:00<00:00, 85.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Loss:  6.4758\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.8612, Avg Loss:  6.0389, Avg Loss1:  5.7057, Avg Loss2:  6.3720: 100%|██████████| 157/157 [00:04<00:00, 34.02it/s]\n",
      "V Loss:  6.0829, Avg Loss:  6.4909, Avg Loss1:  6.2078, Avg Loss2:  6.7741, Counter: 0, Best Loss:  6.4758: 100%|██████████| 32/32 [00:00<00:00, 84.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Loss:  6.4909\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.8588, Avg Loss:  6.0145, Avg Loss1:  5.6845, Avg Loss2:  6.3445: 100%|██████████| 157/157 [00:04<00:00, 34.13it/s]\n",
      "V Loss:  6.7094, Avg Loss:  6.4848, Avg Loss1:  6.2120, Avg Loss2:  6.7576, Counter: 1, Best Loss:  6.4758: 100%|██████████| 32/32 [00:00<00:00, 83.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Loss:  6.4848\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.8558, Avg Loss:  5.9931, Avg Loss1:  5.6626, Avg Loss2:  6.3236: 100%|██████████| 157/157 [00:04<00:00, 34.30it/s]\n",
      "V Loss:  6.3451, Avg Loss:  6.4780, Avg Loss1:  6.1979, Avg Loss2:  6.7582, Counter: 2, Best Loss:  6.4758: 100%|██████████| 32/32 [00:00<00:00, 84.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Loss:  6.4780\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.7883, Avg Loss:  5.9874, Avg Loss1:  5.6383, Avg Loss2:  6.3364: 100%|██████████| 157/157 [00:04<00:00, 33.82it/s]\n",
      "V Loss:  6.2860, Avg Loss:  6.4569, Avg Loss1:  6.1870, Avg Loss2:  6.7267, Counter: 3, Best Loss:  6.4758: 100%|██████████| 32/32 [00:00<00:00, 88.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Loss:  6.4569\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.9093, Avg Loss:  5.9571, Avg Loss1:  5.6176, Avg Loss2:  6.2965: 100%|██████████| 157/157 [00:04<00:00, 34.22it/s]\n",
      "V Loss:  6.3533, Avg Loss:  6.4758, Avg Loss1:  6.1849, Avg Loss2:  6.7666, Counter: 0, Best Loss:  6.4569: 100%|██████████| 32/32 [00:00<00:00, 82.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Loss:  6.4758\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.7294, Avg Loss:  5.9361, Avg Loss1:  5.5939, Avg Loss2:  6.2782: 100%|██████████| 157/157 [00:04<00:00, 34.59it/s]\n",
      "V Loss:  6.1091, Avg Loss:  6.4490, Avg Loss1:  6.1777, Avg Loss2:  6.7203, Counter: 1, Best Loss:  6.4569: 100%|██████████| 32/32 [00:00<00:00, 87.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Loss:  6.4490\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.8467, Avg Loss:  5.9220, Avg Loss1:  5.5772, Avg Loss2:  6.2668: 100%|██████████| 157/157 [00:04<00:00, 34.75it/s]\n",
      "V Loss:  6.7187, Avg Loss:  6.4557, Avg Loss1:  6.1823, Avg Loss2:  6.7291, Counter: 0, Best Loss:  6.4490: 100%|██████████| 32/32 [00:00<00:00, 85.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Loss:  6.4557\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.8715, Avg Loss:  5.9165, Avg Loss1:  5.5558, Avg Loss2:  6.2772: 100%|██████████| 157/157 [00:04<00:00, 34.31it/s]\n",
      "V Loss:  6.0149, Avg Loss:  6.4443, Avg Loss1:  6.1675, Avg Loss2:  6.7211, Counter: 1, Best Loss:  6.4490: 100%|██████████| 32/32 [00:00<00:00, 85.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Loss:  6.4443\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  6.0206, Avg Loss:  5.9073, Avg Loss1:  5.5391, Avg Loss2:  6.2755: 100%|██████████| 157/157 [00:04<00:00, 34.27it/s]\n",
      "V Loss:  6.5106, Avg Loss:  6.4494, Avg Loss1:  6.1747, Avg Loss2:  6.7241, Counter: 0, Best Loss:  6.4443: 100%|██████████| 32/32 [00:00<00:00, 85.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Loss:  6.4494\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.7627, Avg Loss:  5.8801, Avg Loss1:  5.5153, Avg Loss2:  6.2449: 100%|██████████| 157/157 [00:04<00:00, 34.19it/s]\n",
      "V Loss:  6.3778, Avg Loss:  6.4526, Avg Loss1:  6.1657, Avg Loss2:  6.7395, Counter: 1, Best Loss:  6.4443: 100%|██████████| 32/32 [00:00<00:00, 84.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Loss:  6.4526\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.8192, Avg Loss:  5.8639, Avg Loss1:  5.4981, Avg Loss2:  6.2297: 100%|██████████| 157/157 [00:04<00:00, 34.01it/s]\n",
      "V Loss:  6.5331, Avg Loss:  6.4436, Avg Loss1:  6.1627, Avg Loss2:  6.7245, Counter: 2, Best Loss:  6.4443: 100%|██████████| 32/32 [00:00<00:00, 86.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | Loss:  6.4436\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T Loss:  5.8883, Avg Loss:  5.8579, Avg Loss1:  5.4819, Avg Loss2:  6.2339: 100%|██████████| 157/157 [00:04<00:00, 34.16it/s]\n",
      "V Loss:  6.4844, Avg Loss:  6.4513, Avg Loss1:  6.1642, Avg Loss2:  6.7384, Counter: 3, Best Loss:  6.4443: 100%|██████████| 32/32 [00:00<00:00, 82.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Loss:  6.4513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lm.fit(dev_train_loader, dev_validation_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE, filename='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
